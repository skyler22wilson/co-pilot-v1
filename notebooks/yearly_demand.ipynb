{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [17471, 322]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m mask \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonths_no_sale\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m\n\u001b[1;32m     28\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;241m~\u001b[39mmask][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrolling_12m_sales\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Define preprocessing pipeline\u001b[39;00m\n\u001b[1;32m     33\u001b[0m numerical_features \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[0;32m~/miniconda3/envs/partsmatch/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/partsmatch/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2775\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2773\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2775\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2777\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2778\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2779\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2780\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/partsmatch/lib/python3.11/site-packages/sklearn/utils/validation.py:517\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <3x1 sparse matrix ...>]\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    516\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 517\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/partsmatch/lib/python3.11/site-packages/sklearn/utils/validation.py:460\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    458\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    463\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [17471, 322]"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error, make_scorer\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from joblib import dump\n",
    "import shap\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/data/processed_data/parts_data_prepared.csv')\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in {'part_number', 'description', 'supplier_name',\n",
    "                                                         'sales_last_jan','sales_last_feb', 'sales_last_mar', 'sales_last_apr', 'sales_last_may',\n",
    "                                                         'sales_last_jun', 'sales_last_jul', 'sales_last_aug', 'sales_last_sep',\n",
    "                                                         'sales_last_oct', 'sales_last_nov', 'sales_last_dec', 'sales_jan',\n",
    "                                                         'sales_feb', 'sales_mar', 'sales_apr', 'sales_may', 'sales_jun', \n",
    "                                                         'sales_jul', 'sales_aug', 'sales_sep', 'sales_oct', 'sales_nov', \n",
    "                                                         'sales_dec', 'sales_revenue', 'cogs', 'price', 'rolling_12m_sales'}]\n",
    "\n",
    "mask = df['months_no_sale'] >=12                          \n",
    "X = df[~mask][feature_cols]\n",
    "\n",
    "y = df[~mask]['rolling_12m_sales']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', RobustScaler()),\n",
    "            ('power_trans', PowerTransformer(method='yeo-johnson'))]),\n",
    "        numerical_features)\n",
    "    ])\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Define space for Hyperopt\n",
    "space = {\n",
    "    'objective': 'reg:pseudohubererror',\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0.25, 1.0),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.05)),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 15, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 3, 15, 1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 350, 750, 10),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.0001), np.log(1)),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', np.log(1), np.log(3)),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'max_delta_step': hp.quniform('max_delta_step', 5, 10, 1),\n",
    "    'huber_slope': hp.uniform('huber_slope', 0.2, 0.3),\n",
    "}\n",
    "\n",
    "# Set up KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Set up the scoring function for cross-validation\n",
    "scorer = make_scorer(mean_squared_log_error, greater_is_better=False)\n",
    "\n",
    "def objective(params):\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    params['max_delta_step'] = int(params['max_delta_step']) \n",
    "    \n",
    "    model = XGBRegressor(**params)\n",
    "    # Store the scores for each fold using MSLE scorer\n",
    "    scores = cross_val_score(model, X_train_transformed, y_train, scoring=scorer, cv=5)\n",
    "    return {'loss': -scores.mean(), 'status': STATUS_OK}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best_hyperparams = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=500, trials=trials)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "\n",
    "# Correctly casting the best hyperparameters to their correct types\n",
    "best_hyperparams['n_estimators'] = int(best_hyperparams['n_estimators'])\n",
    "best_hyperparams['max_depth'] = int(best_hyperparams['max_depth'])\n",
    "best_hyperparams['max_delta_step'] = int(best_hyperparams['max_delta_step'])\n",
    "best_hyperparams['min_child_weight'] = int(best_hyperparams['min_child_weight'])\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = XGBRegressor(**best_hyperparams)\n",
    "\n",
    "rfecv = RFECV(estimator=model, step=1, cv=KFold(10), scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Fit RFE\n",
    "rfecv.fit(X_train_transformed, y_train)\n",
    "\n",
    "selected_features_mask = rfecv.support_\n",
    "\n",
    "# Get the ranking of the features\n",
    "feature_ranking = rfecv.ranking_\n",
    "\n",
    "# Get the selected features from your preprocessor\n",
    "selected_features = [feature for feature, selected in zip(numerical_features, selected_features_mask) if selected]\n",
    "\n",
    "# Print the optimal number of features\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "\n",
    "# Save the selected features and the best hyperparameters to a JSON file\n",
    "results_dict = {\n",
    "    \"selected_features\": selected_features,\n",
    "    \"best_hyperparameters\": best_hyperparams\n",
    "}\n",
    "\n",
    "with open('/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/Dashboard/Models/demand_predictor/general_model_details.json', 'w') as fp:\n",
    "    json.dump(results_dict, fp, indent=4)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (negative MAE)\")\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'])\n",
    "plt.show()\n",
    "\n",
    "# Transform the model to use the best features\n",
    "X_train_transformed_rfe = rfecv.transform(X_train_transformed)\n",
    "X_test_transformed_rfe = rfecv.transform(X_test_transformed)\n",
    "\n",
    "model.fit(X_train_transformed_rfe, y_train)\n",
    "\n",
    "# Save the preprocessor and the model for application\n",
    "dump(preprocessor, '/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/dashboard/models/demand_predictor/preprocessor.joblib')\n",
    "dump(model, '/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/dashboard/models/demand_predictor/xgb_regressor_with_selected_features.joblib')\n",
    "\n",
    "# Get the feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "importance_df = pd.DataFrame({'feature': selected_features, 'importance': feature_importances})\n",
    "\n",
    "# Save the feature importances\n",
    "importance_df.to_csv('/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/Dashboard/Models/demand_predictor/feature_importances.csv', index=False)\n",
    "\n",
    "# Use SHAP for analysis\n",
    "explainer = shap.Explainer(model, X_train_transformed_rfe)\n",
    "shap_values = explainer(X_test_transformed_rfe)\n",
    "\n",
    "# Save SHAP values\n",
    "shap_df = pd.DataFrame(shap_values.values, columns=selected_features)\n",
    "shap_df.to_csv('/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/Dashboard/Models/demand_predictor/shap_values.csv', index=False)\n",
    "\n",
    "# Plot SHAP summary\n",
    "shap.summary_plot(shap_values, X_test_transformed_rfe, feature_names=selected_features)\n",
    "\n",
    "# Model predictions and evaluation\n",
    "y_pred = model.predict(X_test_transformed_rfe)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'\\nModel Performance')\n",
    "print(f\"Test MSE: {mse}\")\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "print(f\"Test MAE: {mae}\")\n",
    "print(f\"Test R² Score: {r2}\")\n",
    "\n",
    "# Plot residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors='red')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted Values')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters saved in the JSON file:\n",
      "{'selected_features': ['normalized_seasonal_score', 'quantity', 'months_no_sale', 'margin', 'special_orders_ytd', 'cost_per_unit', 'gross_profit', 'roi', 'rolling_3m_sales', '1m_days_supply', 'turnover', 'sell_through_rate', 'days_of_inventory_outstanding'], 'best_hyperparameters': {'colsample_bytree': 0.5430197098620704, 'gamma': 0.2709124173353241, 'huber_slope': 0.2601606035840498, 'learning_rate': 0.048837795279661096, 'max_delta_step': 10, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 750, 'reg_alpha': 0.2166228090437864, 'reg_lambda': 1.015388986935639, 'subsample': 0.9272755187944987}}\n"
     ]
    }
   ],
   "source": [
    "# Load the hyperparameters from the JSON file\n",
    "with open('/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/Dashboard/Models/demand_predictor/general_model_details.json', 'r') as file:\n",
    "    hyperparameters = json.load(file)\n",
    "\n",
    "# Print the hyperparameters\n",
    "print(\"Hyperparameters saved in the JSON file:\")\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          feature        VIF\n",
      "0                           const  12.488638\n",
      "1       normalized_seasonal_score   2.540796\n",
      "2                        quantity   5.002715\n",
      "3                  months_no_sale   1.079207\n",
      "4            quantity_ordered_ytd   1.064776\n",
      "5                negative_on_hand   1.163495\n",
      "6                          margin   3.376943\n",
      "7              special_orders_ytd   1.041023\n",
      "8                   cost_per_unit   3.106045\n",
      "9                      total_cost   1.280671\n",
      "10              margin_percentage   1.177468\n",
      "11                   gross_profit   1.734802\n",
      "12                            roi   1.043944\n",
      "13               rolling_3m_sales   3.511741\n",
      "14                12m_days_supply   4.011494\n",
      "15                 3m_days_supply   4.446802\n",
      "16                 1m_days_supply   4.611917\n",
      "17                       turnover   1.017243\n",
      "18                    3m_turnover   1.044536\n",
      "19              sell_through_rate   1.128851\n",
      "20  days_of_inventory_outstanding   1.441075\n",
      "21           order_to_sales_ratio   1.030547\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/data/processed_data/parts_data_prepared.csv\")\n",
    "\n",
    "# Select the features (X) and target (y) for modeling\n",
    "feature_cols = [col for col in df.columns if col not in {'part_number', 'description', 'supplier_name',\n",
    "                                                         'sales_last_jan','sales_last_feb', 'sales_last_mar', 'sales_last_apr', 'sales_last_may',\n",
    "                                                         'sales_last_jun', 'sales_last_jul', 'sales_last_aug', 'sales_last_sep',\n",
    "                                                         'sales_last_oct', 'sales_last_nov', 'sales_last_dec', 'sales_jan',\n",
    "                                                         'sales_feb', 'sales_mar', 'sales_apr', 'sales_may', 'sales_jun', \n",
    "                                                         'sales_jul', 'sales_aug', 'sales_sep', 'sales_oct', 'sales_nov', \n",
    "                                                         'sales_dec', 'sales_revenue', 'cogs', 'price', 'rolling_12m_sales'}]\n",
    "X = df[feature_cols]\n",
    "\n",
    "# Add a constant to the DataFrame\n",
    "df = sm.add_constant(X)\n",
    "\n",
    "# Calculate VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = df.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "\n",
    "# Display the VIF for each feature\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PartsWise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
