{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('motovan', 0, 0.0, 1.0, 0, -100.0, 0.0, 13, 0, 0, 0.0, 0.0, 9.99, 6, 'obsolete', 1, 0.0, 0.0, '004-153', 0, 0.0, 0.0, 'bearing ntn 6203llu/2a 40x17x12', 0.0)\n",
      "('thibault canada', 0, 1.0, 0.75, 0, -45.99, 0.0052214272, 5, 0, 0, 0.0, 0.0, 4.99, 3, 'non-essential', 1, 0.0, 0.0, '004hf113', 0, 0.0, 360.0, 'hi-flo o-filt hon 15412-hm5-a1', 1.0)\n",
      "('thibault canada', 0, 1.0, 0.7756696429, 0, 0.11, 0.2523689809, 8, 0, 0, 0.0, 0.0, 18.99, 9, 'nearing_obsolete', 1, 0.0, 0.0, '0069922bc', 0, 0.0, 360.0, 'new style universal cruise ctr', 1.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['parts', 'sales']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text, inspect\n",
    "from llama_index.core import SQLDatabase\n",
    "# Path to your database file\n",
    "db_file_path = r\"C:\\Users\\vivia\\co-pilot-v1\\data\\databases\\partswise_island_moto.db\"\n",
    "# \"/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/data/databases/partswise_island_moto.db\"\n",
    "\n",
    "# Create an engine instance\n",
    "connection_string = f\"sqlite:///{db_file_path}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Test the connection using raw SQL\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT * FROM parts LIMIT 3\"))\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "tables = ['sales', 'parts']\n",
    "# sql_database = SQLDatabase(engine, include_tables=tables,sample_rows_in_table_info=5)\n",
    "sql_database = SQLDatabase(engine, sample_rows_in_table_info=2)#by default3 (actually)\n",
    "list(sql_database._all_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   table_name              column_name\n",
      "0       parts            supplier_name\n",
      "1       parts     quantity_ordered_ytd\n",
      "2       parts     sales_to_stock_ratio\n",
      "3       parts        obsolescence_risk\n",
      "4       parts       special_orders_ytd\n",
      "5       parts                      roi\n",
      "6       parts                   demand\n",
      "7       parts           months_no_sale\n",
      "8       parts             safety_stock\n",
      "9       parts            reorder_point\n",
      "10      parts  three_month_days_supply\n",
      "11      parts       one_month_turnover\n",
      "12      parts                    price\n",
      "13      parts            cost_per_unit\n",
      "14      parts       inventory_category\n",
      "15      parts                 quantity\n",
      "16      parts    one_month_days_supply\n",
      "17      parts     three_month_turnover\n",
      "18      parts              part_number\n",
      "19      parts         negative_on_hand\n",
      "20      parts     order_to_sales_ratio\n",
      "21      parts       annual_days_supply\n",
      "22      parts              description\n",
      "23      parts          annual_turnover\n",
      "24      sales                       id\n",
      "25      sales              part_number\n",
      "26      sales                    month\n",
      "27      sales                     year\n",
      "28      sales            quantity_sold\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import inspect\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Database Path\n",
    "db_file_path = r\"C:\\Users\\vivia\\co-pilot-v1\\data\\databases\\partswise_island_moto.db\"\n",
    "#  \"/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/data/databases/partswise_island_moto.db\"\n",
    "engine = create_engine(f\"sqlite:///{db_file_path}\")\n",
    "\n",
    "# Create an inspector object\n",
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "# Create a DataFrame to hold table and column information\n",
    "table_column_df = pd.DataFrame(columns=[\"table_name\", \"column_name\"])\n",
    "\n",
    "# Iterate through the table names and collect column info\n",
    "for table_name in table_names:\n",
    "    table_cols = inspector.get_columns(table_name)  # Use inspector to get columns\n",
    "    table_col_tuples = [(table_name, col['name']) for col in table_cols]\n",
    "    temp_df = pd.DataFrame(table_col_tuples, columns=[\"table_name\", \"column_name\"])\n",
    "    table_column_df = pd.concat([table_column_df, temp_df], ignore_index=True)\n",
    "\n",
    "# Display the table and column names\n",
    "print(table_column_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:17:39,224 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.EMBEDDING ->  0.426969 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:17:39,463 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 10:17:39,505 - INFO - > Table desc str: Inventory categories: essential, non-essential, nearing obsolescence, obsolete. Ensure detailed, relevant responses, including 'supplier_name', 'price', and 'quantity'. Access 'supplier_name' flexibly e.g., ('%bmw'). Convert percentages to decimals (e.g., '50%' as '0.5'). Use JOINs prefaced with table names for combining multiple tables. Calculate COGS as the sum of costs directly associated with goods sold. Calculate Gross Margin Percentage/Gross Margin as (Sales Revenue - COGS) / Sales Revenue * 100.\n",
      "\n",
      "Table 'sales' has columns: id (INTEGER), part_number (VARCHAR), month (VARCHAR), year (INTEGER), quantity_sold (INTEGER), and foreign keys: ['part_number'] -> parts.['part_number']. The table description is: Provides time-based sales data for individual parts. Use for part-specific sales queries.\n",
      "\n",
      "Table 'parts' has columns: supplier_name (TEXT), quantity_ordered_ytd (BIGINT), sales_to_stock_ratio (FLOAT), obsolescence_risk (FLOAT), special_orders_ytd (BIGINT), roi (FLOAT), demand (FLOAT), months_no_sale (BIGINT), safety_stock (BIGINT), reorder_point (BIGINT), three_month_days_supply (FLOAT), one_month_turnover (FLOAT), price (FLOAT), cost_per_unit (BIGINT), inventory_category (TEXT), quantity (BIGINT), one_month_days_supply (FLOAT), three_month_turnover (FLOAT), part_number (TEXT), negative_on_hand (BIGINT), order_to_sales_ratio (FLOAT), annual_days_supply (FLOAT), description (TEXT), annual_turnover (FLOAT), and foreign keys: . The table description is: Provides detailed inventory data for individual parts. Use part-specific queries. Combine with 'sales' tables for temporal financial performance\n",
      "\n",
      "Table 'parts' has columns: supplier_name (TEXT), quantity_ordered_ytd (BIGINT), sales_to_stock_ratio (FLOAT), obsolescence_risk (FLOAT), special_orders_ytd (BIGINT), roi (FLOAT), demand (FLOAT), months_no_sale (BIGINT), safety_stock (BIGINT), reorder_point (BIGINT), three_month_days_supply (FLOAT), one_month_turnover (FLOAT), price (FLOAT), cost_per_unit (BIGINT), inventory_category (TEXT), quantity (BIGINT), one_month_days_supply (FLOAT), three_month_turnover (FLOAT), part_number (TEXT), negative_on_hand (BIGINT), order_to_sales_ratio (FLOAT), annual_days_supply (FLOAT), description (TEXT), annual_turnover (FLOAT), and foreign keys: . The table description is: Provides detailed inventory data for individual parts. Use part-specific queries. Combine with 'sales' tables for temporal financial performance\n",
      "2024-06-18 10:17:43,114 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 10:17:46,797 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.RETRIEVE ->  0.166162 seconds\n",
      "      |_CBEventType.EMBEDDING ->  0.14376 seconds\n",
      "    |_CBEventType.SYNTHESIZE ->  3.060631 seconds\n",
      "      |_CBEventType.TEMPLATING ->  0.0 seconds\n",
      "      |_CBEventType.LLM ->  3.010566 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:17:46,814 - INFO - SQL QUERY after adjustment: select p.supplier_name, avg((s.quantity_sold * p.price - s.quantity_sold * p.cost_per_unit) / (s.quantity_sold * p.price) * 100) as avg_gross_margin_percentage from sales s join parts p on s.part_number = p.part_number where s.month = 'june' and s.year = 2023 group by p.supplier_name order by avg_gross_margin_percentage desc limit 1;\n",
      "2024-06-18 10:17:46,814 - INFO - SQL: select p.supplier_name, avg((s.quantity_sold * p.price - s.quantity_sold * p.cost_per_unit) / (s.quantity_sold * p.price) * 100) as avg_gross_margin_percentage from sales s join parts p on s.part_number = p.part_number where s.month = 'june' and s.year = 2023 group by p.supplier_name order by avg_gross_margin_percentage desc limit 1;\n",
      "2024-06-18 10:17:46,814 - INFO - SQL QUERY Output: select p.supplier_name, avg((s.quantity_sold * p.price - s.quantity_sold * p.cost_per_unit) / (s.quantity_sold * p.price) * 100) as avg_gross_margin_percentage from sales s join parts p on s.part_number = p.part_number where s.month = 'june' and s.year = 2023 group by p.supplier_name order by avg_gross_margin_percentage desc limit 1;\n",
      "2024-06-18 10:17:46,907 - INFO - Query Result Data: [('lordco', 73.28805624788089)]\n",
      "2024-06-18 10:17:47,087 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 10:17:47,095 - INFO - > Table desc str: Inventory categories: essential, non-essential, nearing obsolescence, obsolete. Ensure detailed, relevant responses, including 'supplier_name', 'price', and 'quantity'. Access 'supplier_name' flexibly e.g., ('%bmw'). Convert percentages to decimals (e.g., '50%' as '0.5'). Use JOINs prefaced with table names for combining multiple tables. Calculate COGS as the sum of costs directly associated with goods sold. Calculate Gross Margin Percentage/Gross Margin as (Sales Revenue - COGS) / Sales Revenue * 100.\n",
      "\n",
      "Table 'sales' has columns: id (INTEGER), part_number (VARCHAR), month (VARCHAR), year (INTEGER), quantity_sold (INTEGER), and foreign keys: ['part_number'] -> parts.['part_number']. The table description is: Provides time-based sales data for individual parts. Use for part-specific sales queries.\n",
      "\n",
      "Table 'parts' has columns: supplier_name (TEXT), quantity_ordered_ytd (BIGINT), sales_to_stock_ratio (FLOAT), obsolescence_risk (FLOAT), special_orders_ytd (BIGINT), roi (FLOAT), demand (FLOAT), months_no_sale (BIGINT), safety_stock (BIGINT), reorder_point (BIGINT), three_month_days_supply (FLOAT), one_month_turnover (FLOAT), price (FLOAT), cost_per_unit (BIGINT), inventory_category (TEXT), quantity (BIGINT), one_month_days_supply (FLOAT), three_month_turnover (FLOAT), part_number (TEXT), negative_on_hand (BIGINT), order_to_sales_ratio (FLOAT), annual_days_supply (FLOAT), description (TEXT), annual_turnover (FLOAT), and foreign keys: . The table description is: Provides detailed inventory data for individual parts. Use part-specific queries. Combine with 'sales' tables for temporal financial performance\n",
      "\n",
      "Table 'parts' has columns: supplier_name (TEXT), quantity_ordered_ytd (BIGINT), sales_to_stock_ratio (FLOAT), obsolescence_risk (FLOAT), special_orders_ytd (BIGINT), roi (FLOAT), demand (FLOAT), months_no_sale (BIGINT), safety_stock (BIGINT), reorder_point (BIGINT), three_month_days_supply (FLOAT), one_month_turnover (FLOAT), price (FLOAT), cost_per_unit (BIGINT), inventory_category (TEXT), quantity (BIGINT), one_month_days_supply (FLOAT), three_month_turnover (FLOAT), part_number (TEXT), negative_on_hand (BIGINT), order_to_sales_ratio (FLOAT), annual_days_supply (FLOAT), description (TEXT), annual_turnover (FLOAT), and foreign keys: . The table description is: Provides detailed inventory data for individual parts. Use part-specific queries. Combine with 'sales' tables for temporal financial performance\n",
      "2024-06-18 10:17:50,042 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 10:17:51,405 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.RETRIEVE ->  0.182575 seconds\n",
      "      |_CBEventType.EMBEDDING ->  0.17656 seconds\n",
      "    |_CBEventType.TEMPLATING ->  0.0 seconds\n",
      "    |_CBEventType.LLM ->  2.953325 seconds\n",
      "    |_CBEventType.SYNTHESIZE ->  1.307737 seconds\n",
      "      |_CBEventType.TEMPLATING ->  0.0 seconds\n",
      "      |_CBEventType.LLM ->  1.29953 seconds\n",
      "**********\n",
      "The supplier with the highest average gross margin percentage for the month of June 2023 is Lordco, with an average of 73.29%.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import OperationalError\n",
    "import pandas as pd\n",
    "import logging\n",
    "from llama_index.core import SQLDatabase\n",
    "from llama_index.core.objects import (\n",
    "    SQLTableNodeMapping,\n",
    "    ObjectIndex,\n",
    "    SQLTableSchema,\n",
    ")\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.indices.struct_store import SQLTableRetrieverQueryEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import openai\n",
    "import cProfile\n",
    "import pstats\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Database Path\n",
    "db_file_path = r\"C:\\Users\\vivia\\co-pilot-v1\\data\\databases\\partswise_island_moto.db\"\n",
    "#  \"/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/data/databases/partswise_island_moto.db\"\n",
    "engine = create_engine(f\"sqlite:///{db_file_path}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def setup_nlsql_query_engine():\n",
    "    # Function to initialize SQLDatabase and table objects\n",
    "    def initialize_table_objects():\n",
    "        sql_database = SQLDatabase(engine, sample_rows_in_table_info=2, include_tables=['sales', 'parts'])\n",
    "        parts_context = \"Provides detailed inventory data for individual parts. Use part-specific queries. Combine with 'sales' tables for temporal financial performance\"\n",
    "        sales_context = \"Provides time-based sales data for individual parts. Use for part-specific sales queries.\"\n",
    "\n",
    "        table_node_mapping = SQLTableNodeMapping(sql_database)\n",
    "        table_schema_objs = [\n",
    "            SQLTableSchema(table_name='sales', context_str=sales_context),\n",
    "            SQLTableSchema(table_name='parts', context_str=parts_context),\n",
    "        ]\n",
    "        obj_index = ObjectIndex.from_objects(\n",
    "            table_schema_objs,\n",
    "            table_node_mapping,\n",
    "            VectorStoreIndex,\n",
    "        )\n",
    "        return sql_database, table_schema_objs, obj_index\n",
    "\n",
    "\n",
    "    # Function to generate table context string\n",
    "    def get_table_context_str(sql_database, table_schema_objs):\n",
    "        context_strs = []\n",
    "        for table_schema_obj in table_schema_objs:\n",
    "            table_info = sql_database.get_single_table_info(table_schema_obj.table_name)\n",
    "            if table_schema_obj.context_str:\n",
    "                table_opt_context = \" The table description is: \"\n",
    "                table_opt_context += table_schema_obj.context_str\n",
    "                table_info += table_opt_context\n",
    "            context_strs.append(table_info)\n",
    "        return \"\\n\\n\".join(context_strs)\n",
    "\n",
    "\n",
    "    # Initialize table objects and get table context string\n",
    "    sql_database, table_schema_objs, obj_index = initialize_table_objects()\n",
    "    table_context_str = get_table_context_str(sql_database, table_schema_objs)\n",
    "\n",
    "    # General Context String\n",
    "    context_str = (\n",
    "    \"Inventory categories: essential, non-essential, nearing obsolescence, obsolete. \"\n",
    "    \"Ensure detailed, relevant responses, including 'supplier_name', 'price', and 'quantity'. \"\n",
    "    \"Access 'supplier_name' flexibly e.g., ('%bmw'). \"\n",
    "    \"Convert percentages to decimals (e.g., '50%' as '0.5'). \"\n",
    "    \"Use JOINs prefaced with table names for combining multiple tables. \"\n",
    "    \"Calculate COGS as the sum of costs directly associated with goods sold. \"\n",
    "    \"Calculate Gross Margin Percentage/Gross Margin as (Sales Revenue - COGS) / Sales Revenue * 100.\"\n",
    ")\n",
    "\n",
    "    # Combine Table Contexts\n",
    "    context_str_combined = context_str + \"\\n\\n\" + table_context_str\n",
    "\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]  # Replace with your OpenAI API key\n",
    "    query_engine = SQLTableRetrieverQueryEngine(\n",
    "        sql_database=sql_database,\n",
    "        table_retriever=obj_index.as_retriever(similarity_top_k=1),\n",
    "        synthesize_response=True,\n",
    "        llm=OpenAI(temperature=0.1, model=\"gpt-3.5-turbo-0125\"),\n",
    "        context_str_prefix=context_str_combined\n",
    "    )\n",
    "    return query_engine\n",
    "\n",
    "query_engine = setup_nlsql_query_engine()\n",
    "\n",
    "def process_user_input_to_sql(user_input):\n",
    "    response = query_engine.query(user_input)\n",
    "    sql_query = response.metadata.get('sql_query', '').replace('\\n', ' ').replace('\\r', ' ').strip().lower()\n",
    "    logging.info(f\"SQL QUERY after adjustment: {sql_query}\")\n",
    "    if sql_query.startswith('sql'):\n",
    "        sql_query = sql_query[3:].strip()\n",
    "    logging.info(f\"SQL: {sql_query}\")\n",
    "    return sql_query\n",
    "\n",
    "# This function decides the output format based on whether the SQL query contains aggregation functions\n",
    "def query_output(user_input):\n",
    "    sql_query = process_user_input_to_sql(user_input)\n",
    "    logging.info(f\"SQL QUERY Output: {sql_query}\")\n",
    "\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(sql_query))\n",
    "        result_data = result.fetchall()  # Fetch data once\n",
    "        logging.info(f\"Query Result Data: {result_data}\")\n",
    "        if len(result_data) >= 5:\n",
    "            result_df = pd.DataFrame(result_data, columns=result.keys())\n",
    "            return result_df\n",
    "        else:\n",
    "            # In this case, no table data is available, hence set 'has_data' to False\n",
    "            response = query_engine.query(sql_query)\n",
    "            return str(response)\n",
    "def main():\n",
    "    user_input = \"what brand has the highest average gross margin percentage in June 2023?\"\n",
    "    response = query_output(user_input)\n",
    "    print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $8327.22 in September 2023\n",
    "# $8327.22 in January 2024\n",
    "# $8,327.22 in March 2024\n",
    "\n",
    "#Something is going on here --> this is wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Functions that are used to analyze inventory data and assess problem areas --> create tools from these functions\n",
    "\n",
    "##### Key Problem Areas:\n",
    "- High months no sale: stocked parts are not selling --> pricing issue, quantity issue, poor ordering, or cyclicality?\n",
    "- Improper quantity: quantity below reorder point w/ no current orders --> poor management or long lead time?\n",
    "- Large negative on hand: selling parts we dont have --> poor stocking\n",
    "- Margin/pricing issues: low margin + high sales = need to increase price and vice-versa\n",
    "- Large percentage of obsolescence: need to blow off these parts --> sell at loss to re-coup invested capital\n",
    "- Low ROI: either the parts are not selling or they are too expensive to hold in inventory and should be ordered just-in-time\n",
    "- Special orders with no sales: Could mean we arent charging the customer before ordering or special ordering parts we shouldnt\n",
    "- Stockouts of high sales volume parts: indicates a stockout of parts that have lots of sales --> poor inventory managment\n",
    "- high day supply \n",
    "- High carrying cost\n",
    "\n",
    "##### Define thresholds\n",
    "- Margin below 40% but sales greater than the avg 12 month rolling sales for non-obsolete parts\n",
    "- ROI below 25%\n",
    "- Day supply greater than 65 days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knowledge database build\n",
    "#design: problem --> solution --> reference(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from llama_index.core.tools import FunctionTool, QueryEngineTool, ToolMetadata\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your database file\n",
    "db_file_path = r\"C:\\Users\\vivia\\co-pilot-v1\\data\\databases\\partswise_island_moto.db\"\n",
    "# \"/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/data/databases/partswise_island_moto.db\"\n",
    "connection_string = f\"sqlite:///{db_file_path}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "def analyze_roi(threshold=25):\n",
    "    with engine.connect() as connection:\n",
    "        query = text(\"\"\"\n",
    "            SELECT\n",
    "                part_number,\n",
    "                description, \n",
    "                quantity,\n",
    "                price,     \n",
    "                roi\n",
    "            FROM\n",
    "                parts p\n",
    "            WHERE roi < :threshold\n",
    "        \"\"\")\n",
    "        result = connection.execute(query, {'threshold': threshold})\n",
    "        low_roi_parts = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    return low_roi_parts\n",
    "\n",
    "def analyze_inventory():\n",
    "    with engine.connect() as connection:\n",
    "        query = text(\"\"\"\n",
    "            SELECT \n",
    "                part_number,\n",
    "                description,\n",
    "                quantity,\n",
    "                price,\n",
    "                inventory_category\n",
    "            FROM parts\n",
    "            WHERE inventory_category = 'obsolete'\n",
    "        \"\"\")\n",
    "        result = connection.execute(query)\n",
    "        obsolete_parts = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    return obsolete_parts\n",
    "\n",
    "def analyze_days_supply(threshold=60):\n",
    "    with engine.connect() as connection:\n",
    "        query = text(\"\"\"\n",
    "            SELECT \n",
    "                part_number,\n",
    "                description,\n",
    "                quantity,\n",
    "                price,\n",
    "                inventory_category,\n",
    "                annual_days_supply\n",
    "            FROM parts\n",
    "            WHERE inventory_category != 'obsolete'\n",
    "            AND annual_days_supply > :threshold\n",
    "        \"\"\")\n",
    "        result = connection.execute(query, {'threshold': threshold})\n",
    "        high_days_supply = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    return high_days_supply \n",
    "\n",
    "def analyze_special_orders():\n",
    "    with engine.connect() as connection:\n",
    "        query = text(\"\"\"\n",
    "            SELECT\n",
    "                p.part_number,\n",
    "                p.description,\n",
    "                p.quantity,\n",
    "                p.price,\n",
    "                p.special_orders_ytd, \n",
    "                SUM(s.quantity_sold) as total_quantity_sold\n",
    "            FROM parts p\n",
    "            JOIN sales s ON p.part_number = s.part_number\n",
    "            WHERE p.special_orders_ytd > 0\n",
    "            GROUP BY p.part_number, p.description, p.quantity, p.price, p.special_orders_ytd\n",
    "            HAVING SUM(s.quantity_sold) = 0\n",
    "        \"\"\")\n",
    "        result = connection.execute(query)\n",
    "        special_orders = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    return special_orders\n",
    "\n",
    "def analyze_stockouts(threshold_value=10):\n",
    "    query = text(\"\"\"\n",
    "        WITH PreviousMonthSales AS (\n",
    "            SELECT\n",
    "                part_number,\n",
    "                month,\n",
    "                year,\n",
    "                quantity_sold,\n",
    "                LEAD(quantity_sold) OVER (PARTITION BY part_number ORDER BY year, month) AS next_month_sales,\n",
    "                LEAD(month) OVER (PARTITION BY part_number ORDER BY year, month) AS next_month,\n",
    "                LEAD(year) OVER (PARTITION BY part_number ORDER BY year, month) AS next_year\n",
    "            FROM sales\n",
    "        ),\n",
    "        PotentialStockouts AS (\n",
    "            SELECT\n",
    "                part_number,\n",
    "                month AS previous_month,\n",
    "                year AS previous_year,\n",
    "                quantity_sold AS previous_month_sales,\n",
    "                next_month,\n",
    "                next_year,\n",
    "                next_month_sales AS current_month_sales\n",
    "            FROM PreviousMonthSales\n",
    "            WHERE quantity_sold > :high_sales_threshold\n",
    "            AND (next_month_sales IS NULL OR next_month_sales = 0)\n",
    "        )\n",
    "        SELECT\n",
    "            p.part_number,\n",
    "            p.description,\n",
    "            p.quantity,\n",
    "            p.price,\n",
    "            ps.previous_month,\n",
    "            ps.previous_year,\n",
    "            ps.previous_month_sales,\n",
    "            ps.next_month,\n",
    "            ps.next_year,\n",
    "            ps.current_month_sales\n",
    "        FROM\n",
    "            parts p\n",
    "        JOIN PotentialStockouts ps ON p.part_number = ps.part_number\n",
    "    \"\"\")\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(query, {'high_sales_threshold': threshold_value})\n",
    "        result_df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    return result_df\n",
    "\n",
    "def analyze_negative_on_hand():\n",
    "    query = text(\"\"\"\n",
    "        SELECT\n",
    "            part_number,\n",
    "            description,\n",
    "            quantity,\n",
    "            price, \n",
    "            negative_on_hand\n",
    "        FROM parts\n",
    "        WHERE negative_on_hand != 0\n",
    "    \"\"\")\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(query)\n",
    "        negative_on_hand_parts = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    return negative_on_hand_parts\n",
    "\n",
    "def compile_analysis_results():\n",
    "    results = {}\n",
    "    results['low_roi_parts'] = analyze_roi()\n",
    "    results['obsolete_parts'] = analyze_inventory()\n",
    "    results['high_days_supply_parts'] = analyze_days_supply()\n",
    "    results['special_orders'] = analyze_special_orders()\n",
    "    results['potential_stockouts'] = analyze_stockouts()\n",
    "    results['negative_on_hand_parts'] = analyze_negative_on_hand()\n",
    "    return results\n",
    "\n",
    "# Compile the analysis results\n",
    "results = compile_analysis_results()\n",
    "\n",
    "\n",
    "#need to implement the knowledge database to provide strategic advice based on the compiled analysis\n",
    "\n",
    "    \n",
    "#Other tools for the co-pilot\n",
    "\n",
    "def get_current_year_month():\n",
    "    \"\"\"\n",
    "    Get the current year and month. For temporal queries like: \"how many sales of part 123456 have sold this year so far?\"\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the current year and month.\n",
    "    \"\"\"\n",
    "    current_date = datetime.now()\n",
    "    return current_date.year, current_date.month\n",
    "\n",
    "date_tool = FunctionTool.from_defaults(fn=get_current_year_month, name=\"date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:17:53,498 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.EMBEDDING ->  0.436248 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "# Making tools from functions that analyze the data\n",
    "\n",
    "analyze_roi_tool = FunctionTool.from_defaults(fn=analyze_roi, name=\"roi\")\n",
    "analyze_inventory_tool = FunctionTool.from_defaults(fn=analyze_inventory, name=\"inventory\")\n",
    "analyze_days_supply_tool = FunctionTool.from_defaults(fn=analyze_days_supply, name=\"days_supply\")\n",
    "analyze_special_orders_tool = FunctionTool.from_defaults(fn=analyze_special_orders, name=\"special_orders\")\n",
    "analyze_stockouts_tool = FunctionTool.from_defaults(fn=analyze_stockouts, name=\"stockouts\")\n",
    "analyze_negative_on_hand_tool = FunctionTool.from_defaults(fn=analyze_negative_on_hand, name=\"negative_on_hand\")\n",
    "compile_analysis_results_tool = FunctionTool.from_defaults(fn=compile_analysis_results, name=\"analysis_results\")\n",
    "\n",
    "all_tools = [analyze_roi_tool] + [analyze_inventory_tool] + [analyze_days_supply_tool] + [analyze_special_orders_tool] + [analyze_stockouts_tool] + [analyze_negative_on_hand_tool] + [compile_analysis_results_tool] + [date_tool]\n",
    "all_tools_map = {t.metadata.name: t for t in all_tools}\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:18:06,491 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 10:18:13,007 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Some of the parts that are obsolete in the inventory are:\n",
      "1. Part Number: 004-153, Description: Bearing NTN 6203LLU/2A 40x17x12\n",
      "2. Part Number: 01-0140, Description: Valve Stem Seal\n",
      "3. Part Number: 01-04249, Description: Shinko 804 Big Block 90/90-21\n",
      "4. Part Number: 010088hh, Description: EBC Pad FA88HH Ferodo 310-\n",
      "5. Part Number: 010095hh, Description: EBC Pad FA95HH Ferodo 310-\n",
      "6. Part Number: y12n5.5a-3b, Description: 12N5.5A-3B Conventional 12 Volt\n",
      "7. Part Number: yb5l-b, Description: YB5L-B Yumicron 12 Volt\n",
      "8. Part Number: ytx7l-bs, Description: YTX7L-BS w/Acid APCK\n",
      "9. Part Number: z1-link, Description: Timing Chain Master Link\n",
      "10. Part Number: ze53-0130, Description: Risers\n",
      "\n",
      "These parts are marked as obsolete in the inventory.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "agent = OpenAIAgent.from_tools(all_tools,\n",
    "                               llm=OpenAI(temperature=0.1, model=\"gpt-3.5-turbo-0125\"),\n",
    "                               verbose=False)\n",
    "\n",
    "while True:\n",
    "    text_input = input(\"User:\")\n",
    "    if text_input == \"exit\":\n",
    "        break\n",
    "    response = agent.chat(text_input)\n",
    "    print(f\"Agent: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 16:50:03,563 - WARNING - Ignoring wrong pointing object 22 0 (offset 0)\n",
      "2024-06-18 16:50:03,808 - WARNING - Ignoring wrong pointing object 20 0 (offset 0)\n",
      "2024-06-18 16:50:03,928 - WARNING - Ignoring wrong pointing object 8 0 (offset 0)\n",
      "2024-06-18 16:50:03,938 - WARNING - Ignoring wrong pointing object 10 0 (offset 0)\n",
      "2024-06-18 16:50:03,938 - WARNING - Ignoring wrong pointing object 13 0 (offset 0)\n",
      "2024-06-18 16:50:03,943 - WARNING - Ignoring wrong pointing object 16 0 (offset 0)\n",
      "2024-06-18 16:50:03,945 - WARNING - Ignoring wrong pointing object 18 0 (offset 0)\n",
      "2024-06-18 16:50:03,945 - WARNING - Ignoring wrong pointing object 21 0 (offset 0)\n",
      "2024-06-18 16:50:03,945 - WARNING - Ignoring wrong pointing object 29 0 (offset 0)\n",
      "2024-06-18 16:50:04,056 - WARNING - Ignoring wrong pointing object 6 0 (offset 0)\n",
      "2024-06-18 16:50:04,064 - WARNING - Ignoring wrong pointing object 8 0 (offset 0)\n",
      "2024-06-18 16:50:04,064 - WARNING - Ignoring wrong pointing object 10 0 (offset 0)\n",
      "2024-06-18 16:50:04,070 - WARNING - Ignoring wrong pointing object 23 0 (offset 0)\n",
      "2024-06-18 16:50:04,197 - WARNING - Ignoring wrong pointing object 9 0 (offset 0)\n",
      "2024-06-18 16:50:04,197 - WARNING - Ignoring wrong pointing object 11 0 (offset 0)\n",
      "2024-06-18 16:50:04,200 - WARNING - Ignoring wrong pointing object 17 0 (offset 0)\n",
      "2024-06-18 16:50:04,204 - WARNING - Ignoring wrong pointing object 21 0 (offset 0)\n",
      "2024-06-18 16:50:04,277 - WARNING - Ignoring wrong pointing object 17 0 (offset 0)\n",
      "2024-06-18 16:50:04,384 - WARNING - Ignoring wrong pointing object 6 0 (offset 0)\n",
      "2024-06-18 16:50:04,384 - WARNING - Ignoring wrong pointing object 8 0 (offset 0)\n",
      "2024-06-18 16:50:04,390 - WARNING - Ignoring wrong pointing object 10 0 (offset 0)\n",
      "2024-06-18 16:50:04,390 - WARNING - Ignoring wrong pointing object 12 0 (offset 0)\n",
      "2024-06-18 16:50:04,390 - WARNING - Ignoring wrong pointing object 15 0 (offset 0)\n",
      "2024-06-18 16:50:04,395 - WARNING - Ignoring wrong pointing object 21 0 (offset 0)\n",
      "2024-06-18 16:50:04,620 - WARNING - Ignoring wrong pointing object 8 0 (offset 0)\n",
      "2024-06-18 16:50:04,620 - WARNING - Ignoring wrong pointing object 23 0 (offset 0)\n",
      "2024-06-18 16:50:04,885 - WARNING - Ignoring wrong pointing object 19 0 (offset 0)\n",
      "2024-06-18 16:50:04,974 - WARNING - Ignoring wrong pointing object 7 0 (offset 0)\n",
      "2024-06-18 16:50:04,975 - WARNING - Ignoring wrong pointing object 14 0 (offset 0)\n",
      "2024-06-18 16:50:04,975 - WARNING - Ignoring wrong pointing object 16 0 (offset 0)\n",
      "2024-06-18 16:50:04,975 - WARNING - Ignoring wrong pointing object 30 0 (offset 0)\n",
      "2024-06-18 16:50:04,975 - WARNING - Ignoring wrong pointing object 32 0 (offset 0)\n",
      "2024-06-18 16:50:04,981 - WARNING - Ignoring wrong pointing object 34 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load file C:\\Users\\vivia\\co-pilot-v1\\data\\knowledge_database\\~$le_inventory_trend.docx with error: File is not a zip file. Skipping...\n",
      "Failed to load file C:\\Users\\vivia\\co-pilot-v1\\data\\knowledge_database\\~$le_stock.docx with error: File is not a zip file. Skipping...\n"
     ]
    }
   ],
   "source": [
    "# file reader for knowledge database\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents_dir = r\"C:\\Users\\vivia\\co-pilot-v1\\data\\knowledge_database\"\n",
    "reader = SimpleDirectoryReader(input_dir=documents_dir)\n",
    "knowledge_documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting tools analysis results into a json\n",
    "# import json\n",
    "\n",
    "# base_path = r\"C:\\Users\\vivia\\co-pilot-v1\\Notebooks\\index\\\\\"\n",
    "# out_file = open(base_path + \"tools_data.json\", \"w\")\n",
    "\n",
    "tools_analysis_results = compile_analysis_results()\n",
    "\n",
    "# converting dataframes into a json object\n",
    "for key in tools_analysis_results.keys():\n",
    "    base_path = r\"C:\\Users\\vivia\\co-pilot-v1\\Notebooks\\tools_data\\\\\"\n",
    "    file_name = key + \".csv\"\n",
    "    tools_analysis_results[key].to_csv(base_path + file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_documents_dir = r\"C:\\Users\\vivia\\co-pilot-v1\\Notebooks\\tools_data\"\n",
    "reader = SimpleDirectoryReader(input_dir=tools_documents_dir)\n",
    "tools_documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump data into json document\n",
    "# json.dump(tools_analysis_results, out_file, indent=6)\n",
    "\n",
    "# out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json reader for tools data\n",
    "# from llama_index.readers.json import JSONReader\n",
    "# json_file = r\"C:\\Users\\vivia\\co-pilot-v1\\Notebooks\\index\\tools_data.json\"\n",
    "# reader = JSONReader()\n",
    "# keyword_documents = reader.load_data(input_file=json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "# from llama_index.core.node_parser import JSONNodeParser\n",
    "\n",
    "# parser = JSONNodeParser()\n",
    "knowledge_nodes = Settings.node_parser.get_nodes_from_documents(knowledge_documents)\n",
    "tools_nodes = Settings.node_parser.get_nodes_from_documents(tools_documents)\n",
    "# json_nodes = parser.get_nodes_from_documents(json_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_storage_context = StorageContext.from_defaults()\n",
    "vector_storage_context.docstore.add_documents(knowledge_nodes)\n",
    "\n",
    "keyword_storage_context = StorageContext.from_defaults()\n",
    "keyword_storage_context.docstore.add_documents(tools_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 16:52:20,111 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 16:52:21,364 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 16:52:22,593 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 16:52:24,025 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 16:52:25,188 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 16:52:26,388 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 16:52:27,509 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 16:52:28,874 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 16:52:29,304 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.EMBEDDING ->  1.43837 seconds\n",
      "    |_CBEventType.EMBEDDING ->  1.212701 seconds\n",
      "    |_CBEventType.EMBEDDING ->  1.21111 seconds\n",
      "    |_CBEventType.EMBEDDING ->  1.482426 seconds\n",
      "    |_CBEventType.EMBEDDING ->  1.14219 seconds\n",
      "    |_CBEventType.EMBEDDING ->  1.047182 seconds\n",
      "    |_CBEventType.EMBEDDING ->  1.506438 seconds\n",
      "    |_CBEventType.EMBEDDING ->  1.093928 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.259746 seconds\n",
      "**********\n",
      "**********\n",
      "Trace: index_construction\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleKeywordTableIndex, VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(tools_nodes, storage_context=keyword_storage_context)\n",
    "keyword_index = SimpleKeywordTableIndex(tools_nodes, storage_context=keyword_storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.core.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    "    KeywordTableSimpleRetriever,\n",
    ")\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        keyword_retriever: KeywordTableSimpleRetriever,\n",
    "        mode: str = \"AND\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._keyword_retriever = keyword_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# define custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=2)\n",
    "keyword_retriever = KeywordTableSimpleRetriever(index=keyword_index)\n",
    "custom_retriever = CustomRetriever(vector_retriever, keyword_retriever)\n",
    "\n",
    "# define response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "custom_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# vector query engine\n",
    "vector_query_engine = RetrieverQueryEngine(\n",
    "    retriever=vector_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "# keyword query engine\n",
    "keyword_query_engine = RetrieverQueryEngine(\n",
    "    retriever=keyword_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 16:59:24,274 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-18 16:59:24,346 - INFO - > Starting query: What are some parts from the roi csv\n",
      "2024-06-18 16:59:24,346 - INFO - query keywords: ['roi', 'parts', 'csv']\n",
      "2024-06-18 16:59:24,346 - INFO - > Extracted keywords: ['parts']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  0.250294 seconds\n",
      "**********\n",
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "response = custom_query_engine.query(\"What are some parts from the roi csv\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PartsWise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
