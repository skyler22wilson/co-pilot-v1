{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('motovan', 0, 0.0, 1.0, 0, -100.0, 0.0, 13, 0, 0, 0.0, 0.0, 9.99, 6, 'obsolete', 1, 0.0, 0.0, '004-153', 0, 0.0, 0.0, 'bearing ntn 6203llu/2a 40x17x12', 0.0)\n",
      "('thibault canada', 0, 1.0, 0.75, 0, -45.99, 0.0052214272, 5, 0, 0, 0.0, 0.0, 4.99, 3, 'non-essential', 1, 0.0, 0.0, '004hf113', 0, 0.0, 360.0, 'hi-flo o-filt hon 15412-hm5-a1', 1.0)\n",
      "('thibault canada', 0, 1.0, 0.7756696429, 0, 0.11, 0.2523689809, 8, 0, 0, 0.0, 0.0, 18.99, 9, 'nearing_obsolete', 1, 0.0, 0.0, '0069922bc', 0, 0.0, 360.0, 'new style universal cruise ctr', 1.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['parts', 'sales']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text, inspect\n",
    "from llama_index.core import SQLDatabase\n",
    "# Path to your database file\n",
    "db_file_path = r\"C:\\Users\\vivia\\co-pilot-v1\\data\\databases\\partswise_island_moto.db\"\n",
    "# \"/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/data/databases/partswise_island_moto.db\"\n",
    "\n",
    "# Create an engine instance\n",
    "connection_string = f\"sqlite:///{db_file_path}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Test the connection using raw SQL\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT * FROM parts LIMIT 3\"))\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "tables = ['sales', 'parts']\n",
    "# sql_database = SQLDatabase(engine, include_tables=tables,sample_rows_in_table_info=5)\n",
    "sql_database = SQLDatabase(engine, sample_rows_in_table_info=2)#by default3 (actually)\n",
    "list(sql_database._all_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   table_name              column_name\n",
      "0       parts            supplier_name\n",
      "1       parts     quantity_ordered_ytd\n",
      "2       parts     sales_to_stock_ratio\n",
      "3       parts        obsolescence_risk\n",
      "4       parts       special_orders_ytd\n",
      "5       parts                      roi\n",
      "6       parts                   demand\n",
      "7       parts           months_no_sale\n",
      "8       parts             safety_stock\n",
      "9       parts            reorder_point\n",
      "10      parts  three_month_days_supply\n",
      "11      parts       one_month_turnover\n",
      "12      parts                    price\n",
      "13      parts            cost_per_unit\n",
      "14      parts       inventory_category\n",
      "15      parts                 quantity\n",
      "16      parts    one_month_days_supply\n",
      "17      parts     three_month_turnover\n",
      "18      parts              part_number\n",
      "19      parts         negative_on_hand\n",
      "20      parts     order_to_sales_ratio\n",
      "21      parts       annual_days_supply\n",
      "22      parts              description\n",
      "23      parts          annual_turnover\n",
      "24      sales                       id\n",
      "25      sales              part_number\n",
      "26      sales                    month\n",
      "27      sales                     year\n",
      "28      sales            quantity_sold\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import inspect\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Database Path\n",
    "db_file_path = r\"C:\\Users\\vivia\\co-pilot-v1\\data\\databases\\partswise_island_moto.db\"\n",
    "#  \"/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/data/databases/partswise_island_moto.db\"\n",
    "engine = create_engine(f\"sqlite:///{db_file_path}\")\n",
    "\n",
    "# Create an inspector object\n",
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "# Create a DataFrame to hold table and column information\n",
    "table_column_df = pd.DataFrame(columns=[\"table_name\", \"column_name\"])\n",
    "\n",
    "# Iterate through the table names and collect column info\n",
    "for table_name in table_names:\n",
    "    table_cols = inspector.get_columns(table_name)  # Use inspector to get columns\n",
    "    table_col_tuples = [(table_name, col['name']) for col in table_cols]\n",
    "    temp_df = pd.DataFrame(table_col_tuples, columns=[\"table_name\", \"column_name\"])\n",
    "    table_column_df = pd.concat([table_column_df, temp_df], ignore_index=True)\n",
    "\n",
    "# Display the table and column names\n",
    "print(table_column_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 09:47:22,621 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.EMBEDDING ->  0.412589 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 09:47:22,806 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 09:47:22,811 - INFO - > Table desc str: Inventory categories: essential, non-essential, nearing obsolescence, obsolete. Ensure detailed, relevant responses, including 'supplier_name', 'price', and 'quantity'. Access 'supplier_name' flexibly e.g., ('%bmw'). Convert percentages to decimals (e.g., '50%' as '0.5'). Use JOINs prefaced with table names for combining multiple tables. Calculate COGS as the sum of costs directly associated with goods sold. Calculate Gross Margin Percentage/Gross Margin as (Sales Revenue - COGS) / Sales Revenue * 100.\n",
      "\n",
      "Table 'sales' has columns: id (INTEGER), part_number (VARCHAR), month (VARCHAR), year (INTEGER), quantity_sold (INTEGER), and foreign keys: ['part_number'] -> parts.['part_number']. The table description is: Provides time-based sales data for individual parts. Use for part-specific sales queries.\n",
      "\n",
      "Table 'parts' has columns: supplier_name (TEXT), quantity_ordered_ytd (BIGINT), sales_to_stock_ratio (FLOAT), obsolescence_risk (FLOAT), special_orders_ytd (BIGINT), roi (FLOAT), demand (FLOAT), months_no_sale (BIGINT), safety_stock (BIGINT), reorder_point (BIGINT), three_month_days_supply (FLOAT), one_month_turnover (FLOAT), price (FLOAT), cost_per_unit (BIGINT), inventory_category (TEXT), quantity (BIGINT), one_month_days_supply (FLOAT), three_month_turnover (FLOAT), part_number (TEXT), negative_on_hand (BIGINT), order_to_sales_ratio (FLOAT), annual_days_supply (FLOAT), description (TEXT), annual_turnover (FLOAT), and foreign keys: . The table description is: Provides detailed inventory data for individual parts. Use part-specific queries. Combine with 'sales' tables for temporal financial performance\n",
      "\n",
      "Table 'parts' has columns: supplier_name (TEXT), quantity_ordered_ytd (BIGINT), sales_to_stock_ratio (FLOAT), obsolescence_risk (FLOAT), special_orders_ytd (BIGINT), roi (FLOAT), demand (FLOAT), months_no_sale (BIGINT), safety_stock (BIGINT), reorder_point (BIGINT), three_month_days_supply (FLOAT), one_month_turnover (FLOAT), price (FLOAT), cost_per_unit (BIGINT), inventory_category (TEXT), quantity (BIGINT), one_month_days_supply (FLOAT), three_month_turnover (FLOAT), part_number (TEXT), negative_on_hand (BIGINT), order_to_sales_ratio (FLOAT), annual_days_supply (FLOAT), description (TEXT), annual_turnover (FLOAT), and foreign keys: . The table description is: Provides detailed inventory data for individual parts. Use part-specific queries. Combine with 'sales' tables for temporal financial performance\n",
      "2024-06-19 09:47:30,280 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 09:47:31,093 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.RETRIEVE ->  0.128312 seconds\n",
      "      |_CBEventType.EMBEDDING ->  0.127298 seconds\n",
      "    |_CBEventType.SYNTHESIZE ->  0.791117 seconds\n",
      "      |_CBEventType.TEMPLATING ->  0.0 seconds\n",
      "      |_CBEventType.LLM ->  0.791117 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 09:47:31,100 - INFO - SQL QUERY after adjustment: select p.supplier_name, avg(((s.quantity_sold * p.price) - (s.quantity_sold * p.cost_per_unit)) / (s.quantity_sold * p.price) * 100) as avg_gross_margin_percentage from sales s join parts p on s.part_number = p.part_number where s.month = 'june' and s.year = 2023 group by p.supplier_name order by avg_gross_margin_percentage desc limit 1;\n",
      "2024-06-19 09:47:31,100 - INFO - SQL: select p.supplier_name, avg(((s.quantity_sold * p.price) - (s.quantity_sold * p.cost_per_unit)) / (s.quantity_sold * p.price) * 100) as avg_gross_margin_percentage from sales s join parts p on s.part_number = p.part_number where s.month = 'june' and s.year = 2023 group by p.supplier_name order by avg_gross_margin_percentage desc limit 1;\n",
      "2024-06-19 09:47:31,100 - INFO - SQL QUERY Output: select p.supplier_name, avg(((s.quantity_sold * p.price) - (s.quantity_sold * p.cost_per_unit)) / (s.quantity_sold * p.price) * 100) as avg_gross_margin_percentage from sales s join parts p on s.part_number = p.part_number where s.month = 'june' and s.year = 2023 group by p.supplier_name order by avg_gross_margin_percentage desc limit 1;\n",
      "2024-06-19 09:47:31,128 - INFO - Query Result Data: [('lordco', 73.28805624788089)]\n",
      "2024-06-19 09:47:31,306 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 09:47:31,306 - INFO - > Table desc str: Inventory categories: essential, non-essential, nearing obsolescence, obsolete. Ensure detailed, relevant responses, including 'supplier_name', 'price', and 'quantity'. Access 'supplier_name' flexibly e.g., ('%bmw'). Convert percentages to decimals (e.g., '50%' as '0.5'). Use JOINs prefaced with table names for combining multiple tables. Calculate COGS as the sum of costs directly associated with goods sold. Calculate Gross Margin Percentage/Gross Margin as (Sales Revenue - COGS) / Sales Revenue * 100.\n",
      "\n",
      "Table 'sales' has columns: id (INTEGER), part_number (VARCHAR), month (VARCHAR), year (INTEGER), quantity_sold (INTEGER), and foreign keys: ['part_number'] -> parts.['part_number']. The table description is: Provides time-based sales data for individual parts. Use for part-specific sales queries.\n",
      "\n",
      "Table 'parts' has columns: supplier_name (TEXT), quantity_ordered_ytd (BIGINT), sales_to_stock_ratio (FLOAT), obsolescence_risk (FLOAT), special_orders_ytd (BIGINT), roi (FLOAT), demand (FLOAT), months_no_sale (BIGINT), safety_stock (BIGINT), reorder_point (BIGINT), three_month_days_supply (FLOAT), one_month_turnover (FLOAT), price (FLOAT), cost_per_unit (BIGINT), inventory_category (TEXT), quantity (BIGINT), one_month_days_supply (FLOAT), three_month_turnover (FLOAT), part_number (TEXT), negative_on_hand (BIGINT), order_to_sales_ratio (FLOAT), annual_days_supply (FLOAT), description (TEXT), annual_turnover (FLOAT), and foreign keys: . The table description is: Provides detailed inventory data for individual parts. Use part-specific queries. Combine with 'sales' tables for temporal financial performance\n",
      "\n",
      "Table 'parts' has columns: supplier_name (TEXT), quantity_ordered_ytd (BIGINT), sales_to_stock_ratio (FLOAT), obsolescence_risk (FLOAT), special_orders_ytd (BIGINT), roi (FLOAT), demand (FLOAT), months_no_sale (BIGINT), safety_stock (BIGINT), reorder_point (BIGINT), three_month_days_supply (FLOAT), one_month_turnover (FLOAT), price (FLOAT), cost_per_unit (BIGINT), inventory_category (TEXT), quantity (BIGINT), one_month_days_supply (FLOAT), three_month_turnover (FLOAT), part_number (TEXT), negative_on_hand (BIGINT), order_to_sales_ratio (FLOAT), annual_days_supply (FLOAT), description (TEXT), annual_turnover (FLOAT), and foreign keys: . The table description is: Provides detailed inventory data for individual parts. Use part-specific queries. Combine with 'sales' tables for temporal financial performance\n",
      "2024-06-19 09:47:36,527 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 09:47:37,779 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.RETRIEVE ->  0.177169 seconds\n",
      "      |_CBEventType.EMBEDDING ->  0.176559 seconds\n",
      "    |_CBEventType.TEMPLATING ->  0.0 seconds\n",
      "    |_CBEventType.LLM ->  5.22082 seconds\n",
      "    |_CBEventType.SYNTHESIZE ->  1.22081 seconds\n",
      "      |_CBEventType.TEMPLATING ->  0.0 seconds\n",
      "      |_CBEventType.LLM ->  1.218476 seconds\n",
      "**********\n",
      "The supplier with the highest average gross margin percentage for the month of June 2023 is Lordco, with an average of 73.29%.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import OperationalError\n",
    "import pandas as pd\n",
    "import logging\n",
    "from llama_index.core import SQLDatabase\n",
    "from llama_index.core.objects import (\n",
    "    SQLTableNodeMapping,\n",
    "    ObjectIndex,\n",
    "    SQLTableSchema,\n",
    ")\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.indices.struct_store import SQLTableRetrieverQueryEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import openai\n",
    "import cProfile\n",
    "import pstats\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Database Path\n",
    "db_file_path = r\"C:\\Users\\vivia\\co-pilot-v1\\data\\databases\\partswise_island_moto.db\"\n",
    "#  \"/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/data/databases/partswise_island_moto.db\"\n",
    "engine = create_engine(f\"sqlite:///{db_file_path}\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def setup_nlsql_query_engine():\n",
    "    # Function to initialize SQLDatabase and table objects\n",
    "    def initialize_table_objects():\n",
    "        sql_database = SQLDatabase(engine, sample_rows_in_table_info=2, include_tables=['sales', 'parts'])\n",
    "        parts_context = \"Provides detailed inventory data for individual parts. Use part-specific queries. Combine with 'sales' tables for temporal financial performance\"\n",
    "        sales_context = \"Provides time-based sales data for individual parts. Use for part-specific sales queries.\"\n",
    "\n",
    "        table_node_mapping = SQLTableNodeMapping(sql_database)\n",
    "        table_schema_objs = [\n",
    "            SQLTableSchema(table_name='sales', context_str=sales_context),\n",
    "            SQLTableSchema(table_name='parts', context_str=parts_context),\n",
    "        ]\n",
    "        obj_index = ObjectIndex.from_objects(\n",
    "            table_schema_objs,\n",
    "            table_node_mapping,\n",
    "            VectorStoreIndex,\n",
    "        )\n",
    "        return sql_database, table_schema_objs, obj_index\n",
    "\n",
    "\n",
    "    # Function to generate table context string\n",
    "    def get_table_context_str(sql_database, table_schema_objs):\n",
    "        context_strs = []\n",
    "        for table_schema_obj in table_schema_objs:\n",
    "            table_info = sql_database.get_single_table_info(table_schema_obj.table_name)\n",
    "            if table_schema_obj.context_str:\n",
    "                table_opt_context = \" The table description is: \"\n",
    "                table_opt_context += table_schema_obj.context_str\n",
    "                table_info += table_opt_context\n",
    "            context_strs.append(table_info)\n",
    "        return \"\\n\\n\".join(context_strs)\n",
    "\n",
    "\n",
    "    # Initialize table objects and get table context string\n",
    "    sql_database, table_schema_objs, obj_index = initialize_table_objects()\n",
    "    table_context_str = get_table_context_str(sql_database, table_schema_objs)\n",
    "\n",
    "    # General Context String\n",
    "    context_str = (\n",
    "    \"Inventory categories: essential, non-essential, nearing obsolescence, obsolete. \"\n",
    "    \"Ensure detailed, relevant responses, including 'supplier_name', 'price', and 'quantity'. \"\n",
    "    \"Access 'supplier_name' flexibly e.g., ('%bmw'). \"\n",
    "    \"Convert percentages to decimals (e.g., '50%' as '0.5'). \"\n",
    "    \"Use JOINs prefaced with table names for combining multiple tables. \"\n",
    "    \"Calculate COGS as the sum of costs directly associated with goods sold. \"\n",
    "    \"Calculate Gross Margin Percentage/Gross Margin as (Sales Revenue - COGS) / Sales Revenue * 100.\"\n",
    ")\n",
    "\n",
    "    # Combine Table Contexts\n",
    "    context_str_combined = context_str + \"\\n\\n\" + table_context_str\n",
    "\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]  # Replace with your OpenAI API key\n",
    "    query_engine = SQLTableRetrieverQueryEngine(\n",
    "        sql_database=sql_database,\n",
    "        table_retriever=obj_index.as_retriever(similarity_top_k=1),\n",
    "        synthesize_response=True,\n",
    "        llm=OpenAI(temperature=0.1, model=\"gpt-3.5-turbo-0125\"),\n",
    "        context_str_prefix=context_str_combined\n",
    "    )\n",
    "    return query_engine\n",
    "\n",
    "query_engine = setup_nlsql_query_engine()\n",
    "\n",
    "def process_user_input_to_sql(user_input):\n",
    "    response = query_engine.query(user_input)\n",
    "    sql_query = response.metadata.get('sql_query', '').replace('\\n', ' ').replace('\\r', ' ').strip().lower()\n",
    "    logging.info(f\"SQL QUERY after adjustment: {sql_query}\")\n",
    "    if sql_query.startswith('sql'):\n",
    "        sql_query = sql_query[3:].strip()\n",
    "    logging.info(f\"SQL: {sql_query}\")\n",
    "    return sql_query\n",
    "\n",
    "# This function decides the output format based on whether the SQL query contains aggregation functions\n",
    "def query_output(user_input):\n",
    "    sql_query = process_user_input_to_sql(user_input)\n",
    "    logging.info(f\"SQL QUERY Output: {sql_query}\")\n",
    "\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(sql_query))\n",
    "        result_data = result.fetchall()  # Fetch data once\n",
    "        logging.info(f\"Query Result Data: {result_data}\")\n",
    "        if len(result_data) >= 5:\n",
    "            result_df = pd.DataFrame(result_data, columns=result.keys())\n",
    "            return result_df\n",
    "        else:\n",
    "            # In this case, no table data is available, hence set 'has_data' to False\n",
    "            response = query_engine.query(sql_query)\n",
    "            return str(response)\n",
    "def main():\n",
    "    user_input = \"what brand has the highest average gross margin percentage in June 2023?\"\n",
    "    response = query_output(user_input)\n",
    "    print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $8327.22 in September 2023\n",
    "# $8327.22 in January 2024\n",
    "# $8,327.22 in March 2024\n",
    "\n",
    "#Something is going on here --> this is wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Functions that are used to analyze inventory data and assess problem areas --> create tools from these functions\n",
    "\n",
    "##### Key Problem Areas:\n",
    "- High months no sale: stocked parts are not selling --> pricing issue, quantity issue, poor ordering, or cyclicality?\n",
    "- Improper quantity: quantity below reorder point w/ no current orders --> poor management or long lead time?\n",
    "- Large negative on hand: selling parts we dont have --> poor stocking\n",
    "- Margin/pricing issues: low margin + high sales = need to increase price and vice-versa\n",
    "- Large percentage of obsolescence: need to blow off these parts --> sell at loss to re-coup invested capital\n",
    "- Low ROI: either the parts are not selling or they are too expensive to hold in inventory and should be ordered just-in-time\n",
    "- Special orders with no sales: Could mean we arent charging the customer before ordering or special ordering parts we shouldnt\n",
    "- Stockouts of high sales volume parts: indicates a stockout of parts that have lots of sales --> poor inventory managment\n",
    "- high day supply \n",
    "- High carrying cost\n",
    "\n",
    "##### Define thresholds\n",
    "- Margin below 40% but sales greater than the avg 12 month rolling sales for non-obsolete parts\n",
    "- ROI below 25%\n",
    "- Day supply greater than 65 days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knowledge database build\n",
    "#design: problem --> solution --> reference(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from llama_index.core.tools import FunctionTool, QueryEngineTool, ToolMetadata\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your database file\n",
    "db_file_path = r\"C:\\Users\\vivia\\co-pilot-v1\\data\\databases\\partswise_island_moto.db\"\n",
    "# \"/Users/skylerwilson/Desktop/PartsWise/co-pilot-v1/data/databases/partswise_island_moto.db\"\n",
    "connection_string = f\"sqlite:///{db_file_path}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "def analyze_roi(threshold=25):\n",
    "    with engine.connect() as connection:\n",
    "        query = text(\"\"\"\n",
    "            SELECT\n",
    "                part_number,\n",
    "                description, \n",
    "                quantity,\n",
    "                price,     \n",
    "                roi\n",
    "            FROM\n",
    "                parts p\n",
    "            WHERE roi < :threshold\n",
    "        \"\"\")\n",
    "        result = connection.execute(query, {'threshold': threshold})\n",
    "        low_roi_parts = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    return low_roi_parts\n",
    "\n",
    "def analyze_inventory():\n",
    "    with engine.connect() as connection:\n",
    "        query = text(\"\"\"\n",
    "            SELECT \n",
    "                part_number,\n",
    "                description,\n",
    "                quantity,\n",
    "                price,\n",
    "                inventory_category\n",
    "            FROM parts\n",
    "            WHERE inventory_category = 'obsolete'\n",
    "        \"\"\")\n",
    "        result = connection.execute(query)\n",
    "        obsolete_parts = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    return obsolete_parts\n",
    "\n",
    "def analyze_days_supply(threshold=60):\n",
    "    with engine.connect() as connection:\n",
    "        query = text(\"\"\"\n",
    "            SELECT \n",
    "                part_number,\n",
    "                description,\n",
    "                quantity,\n",
    "                price,\n",
    "                inventory_category,\n",
    "                annual_days_supply\n",
    "            FROM parts\n",
    "            WHERE inventory_category != 'obsolete'\n",
    "            AND annual_days_supply > :threshold\n",
    "        \"\"\")\n",
    "        result = connection.execute(query, {'threshold': threshold})\n",
    "        high_days_supply = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    return high_days_supply \n",
    "\n",
    "def analyze_special_orders():\n",
    "    with engine.connect() as connection:\n",
    "        query = text(\"\"\"\n",
    "            SELECT\n",
    "                p.part_number,\n",
    "                p.description,\n",
    "                p.quantity,\n",
    "                p.price,\n",
    "                p.special_orders_ytd, \n",
    "                SUM(s.quantity_sold) as total_quantity_sold\n",
    "            FROM parts p\n",
    "            JOIN sales s ON p.part_number = s.part_number\n",
    "            WHERE p.special_orders_ytd > 0\n",
    "            GROUP BY p.part_number, p.description, p.quantity, p.price, p.special_orders_ytd\n",
    "            HAVING SUM(s.quantity_sold) = 0\n",
    "        \"\"\")\n",
    "        result = connection.execute(query)\n",
    "        special_orders = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    return special_orders\n",
    "\n",
    "def analyze_stockouts(threshold_value=10):\n",
    "    query = text(\"\"\"\n",
    "        WITH PreviousMonthSales AS (\n",
    "            SELECT\n",
    "                part_number,\n",
    "                month,\n",
    "                year,\n",
    "                quantity_sold,\n",
    "                LEAD(quantity_sold) OVER (PARTITION BY part_number ORDER BY year, month) AS next_month_sales,\n",
    "                LEAD(month) OVER (PARTITION BY part_number ORDER BY year, month) AS next_month,\n",
    "                LEAD(year) OVER (PARTITION BY part_number ORDER BY year, month) AS next_year\n",
    "            FROM sales\n",
    "        ),\n",
    "        PotentialStockouts AS (\n",
    "            SELECT\n",
    "                part_number,\n",
    "                month AS previous_month,\n",
    "                year AS previous_year,\n",
    "                quantity_sold AS previous_month_sales,\n",
    "                next_month,\n",
    "                next_year,\n",
    "                next_month_sales AS current_month_sales\n",
    "            FROM PreviousMonthSales\n",
    "            WHERE quantity_sold > :high_sales_threshold\n",
    "            AND (next_month_sales IS NULL OR next_month_sales = 0)\n",
    "        )\n",
    "        SELECT\n",
    "            p.part_number,\n",
    "            p.description,\n",
    "            p.quantity,\n",
    "            p.price,\n",
    "            ps.previous_month,\n",
    "            ps.previous_year,\n",
    "            ps.previous_month_sales,\n",
    "            ps.next_month,\n",
    "            ps.next_year,\n",
    "            ps.current_month_sales\n",
    "        FROM\n",
    "            parts p\n",
    "        JOIN PotentialStockouts ps ON p.part_number = ps.part_number\n",
    "    \"\"\")\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(query, {'high_sales_threshold': threshold_value})\n",
    "        result_df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    return result_df\n",
    "\n",
    "def analyze_negative_on_hand():\n",
    "    query = text(\"\"\"\n",
    "        SELECT\n",
    "            part_number,\n",
    "            description,\n",
    "            quantity,\n",
    "            price, \n",
    "            negative_on_hand\n",
    "        FROM parts\n",
    "        WHERE negative_on_hand != 0\n",
    "    \"\"\")\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(query)\n",
    "        negative_on_hand_parts = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    return negative_on_hand_parts\n",
    "\n",
    "def compile_analysis_results():\n",
    "    results = {}\n",
    "    results['low_roi_parts'] = analyze_roi()\n",
    "    results['obsolete_parts'] = analyze_inventory()\n",
    "    results['high_days_supply_parts'] = analyze_days_supply()\n",
    "    results['special_orders'] = analyze_special_orders()\n",
    "    results['potential_stockouts'] = analyze_stockouts()\n",
    "    results['negative_on_hand_parts'] = analyze_negative_on_hand()\n",
    "    return results\n",
    "\n",
    "# Compile the analysis results\n",
    "results = compile_analysis_results()\n",
    "\n",
    "\n",
    "#need to implement the knowledge database to provide strategic advice based on the compiled analysis\n",
    "\n",
    "    \n",
    "#Other tools for the co-pilot\n",
    "\n",
    "def get_current_year_month():\n",
    "    \"\"\"\n",
    "    Get the current year and month. For temporal queries like: \"how many sales of part 123456 have sold this year so far?\"\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the current year and month.\n",
    "    \"\"\"\n",
    "    current_date = datetime.now()\n",
    "    return current_date.year, current_date.month\n",
    "\n",
    "date_tool = FunctionTool.from_defaults(fn=get_current_year_month, name=\"date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 09:47:50,874 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.EMBEDDING ->  0.510137 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "# Making tools from functions that analyze the data\n",
    "\n",
    "analyze_roi_tool = FunctionTool.from_defaults(fn=analyze_roi, name=\"roi\")\n",
    "analyze_inventory_tool = FunctionTool.from_defaults(fn=analyze_inventory, name=\"inventory\")\n",
    "analyze_days_supply_tool = FunctionTool.from_defaults(fn=analyze_days_supply, name=\"days_supply\")\n",
    "analyze_special_orders_tool = FunctionTool.from_defaults(fn=analyze_special_orders, name=\"special_orders\")\n",
    "analyze_stockouts_tool = FunctionTool.from_defaults(fn=analyze_stockouts, name=\"stockouts\")\n",
    "analyze_negative_on_hand_tool = FunctionTool.from_defaults(fn=analyze_negative_on_hand, name=\"negative_on_hand\")\n",
    "compile_analysis_results_tool = FunctionTool.from_defaults(fn=compile_analysis_results, name=\"analysis_results\")\n",
    "\n",
    "all_tools = [analyze_roi_tool] + [analyze_inventory_tool] + [analyze_days_supply_tool] + [analyze_special_orders_tool] + [analyze_stockouts_tool] + [analyze_negative_on_hand_tool] + [compile_analysis_results_tool] + [date_tool]\n",
    "all_tools_map = {t.metadata.name: t for t in all_tools}\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making vector index from knowledge database docs\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_dir = r\"C:\\Users\\vivia\\co-pilot-v1\\data\\knowledge_database\"\n",
    "try:\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=knowledge_dir\n",
    "    )\n",
    "    knowledge_index = load_index_from_storage(storage_context)\n",
    "\n",
    "    index_loaded = True\n",
    "except:\n",
    "    index_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 10:44:51,523 - WARNING - Ignoring wrong pointing object 22 0 (offset 0)\n",
      "2024-06-19 10:44:51,658 - WARNING - Ignoring wrong pointing object 20 0 (offset 0)\n",
      "2024-06-19 10:44:51,771 - WARNING - Ignoring wrong pointing object 8 0 (offset 0)\n",
      "2024-06-19 10:44:51,773 - WARNING - Ignoring wrong pointing object 10 0 (offset 0)\n",
      "2024-06-19 10:44:51,774 - WARNING - Ignoring wrong pointing object 13 0 (offset 0)\n",
      "2024-06-19 10:44:51,774 - WARNING - Ignoring wrong pointing object 16 0 (offset 0)\n",
      "2024-06-19 10:44:51,775 - WARNING - Ignoring wrong pointing object 18 0 (offset 0)\n",
      "2024-06-19 10:44:51,776 - WARNING - Ignoring wrong pointing object 21 0 (offset 0)\n",
      "2024-06-19 10:44:51,779 - WARNING - Ignoring wrong pointing object 29 0 (offset 0)\n",
      "2024-06-19 10:44:51,872 - WARNING - Ignoring wrong pointing object 6 0 (offset 0)\n",
      "2024-06-19 10:44:51,872 - WARNING - Ignoring wrong pointing object 8 0 (offset 0)\n",
      "2024-06-19 10:44:51,872 - WARNING - Ignoring wrong pointing object 10 0 (offset 0)\n",
      "2024-06-19 10:44:51,876 - WARNING - Ignoring wrong pointing object 23 0 (offset 0)\n",
      "2024-06-19 10:44:51,987 - WARNING - Ignoring wrong pointing object 9 0 (offset 0)\n",
      "2024-06-19 10:44:51,989 - WARNING - Ignoring wrong pointing object 11 0 (offset 0)\n",
      "2024-06-19 10:44:51,990 - WARNING - Ignoring wrong pointing object 17 0 (offset 0)\n",
      "2024-06-19 10:44:51,991 - WARNING - Ignoring wrong pointing object 21 0 (offset 0)\n",
      "2024-06-19 10:44:52,053 - WARNING - Ignoring wrong pointing object 17 0 (offset 0)\n",
      "2024-06-19 10:44:52,151 - WARNING - Ignoring wrong pointing object 6 0 (offset 0)\n",
      "2024-06-19 10:44:52,152 - WARNING - Ignoring wrong pointing object 8 0 (offset 0)\n",
      "2024-06-19 10:44:52,154 - WARNING - Ignoring wrong pointing object 10 0 (offset 0)\n",
      "2024-06-19 10:44:52,155 - WARNING - Ignoring wrong pointing object 12 0 (offset 0)\n",
      "2024-06-19 10:44:52,156 - WARNING - Ignoring wrong pointing object 15 0 (offset 0)\n",
      "2024-06-19 10:44:52,157 - WARNING - Ignoring wrong pointing object 21 0 (offset 0)\n",
      "2024-06-19 10:44:52,437 - WARNING - Ignoring wrong pointing object 8 0 (offset 0)\n",
      "2024-06-19 10:44:52,438 - WARNING - Ignoring wrong pointing object 23 0 (offset 0)\n",
      "2024-06-19 10:44:52,703 - WARNING - Ignoring wrong pointing object 19 0 (offset 0)\n",
      "2024-06-19 10:44:52,859 - WARNING - Ignoring wrong pointing object 7 0 (offset 0)\n",
      "2024-06-19 10:44:52,860 - WARNING - Ignoring wrong pointing object 14 0 (offset 0)\n",
      "2024-06-19 10:44:52,861 - WARNING - Ignoring wrong pointing object 16 0 (offset 0)\n",
      "2024-06-19 10:44:52,865 - WARNING - Ignoring wrong pointing object 30 0 (offset 0)\n",
      "2024-06-19 10:44:52,866 - WARNING - Ignoring wrong pointing object 32 0 (offset 0)\n",
      "2024-06-19 10:44:52,866 - WARNING - Ignoring wrong pointing object 34 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load file C:\\Users\\vivia\\co-pilot-v1\\data\\knowledge_database\\~$le_inventory_trend.docx with error: File is not a zip file. Skipping...\n",
      "Failed to load file C:\\Users\\vivia\\co-pilot-v1\\data\\knowledge_database\\~$le_stock.docx with error: File is not a zip file. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 10:44:54,591 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.NODE_PARSING ->  0.692128 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.014509 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.004593 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.001011 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00101 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.001585 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00103 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000989 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.0 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.001 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.001 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.0 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00151 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.0 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.0 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.0 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.002034 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.003003 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.000997 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.0 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.008153 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.001167 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.0 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00102 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.00045 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.001039 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.0 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.001602 seconds\n",
      "      |_CBEventType.CHUNKING ->  0.0 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.703323 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "if not index_loaded:\n",
    "    # load data\n",
    "    knowledge_docs = SimpleDirectoryReader(input_dir=knowledge_dir).load_data()\n",
    "\n",
    "    # build index\n",
    "    knowledge_index = VectorStoreIndex.from_documents(knowledge_docs)\n",
    "\n",
    "    # persist index\n",
    "    knowledge_index.storage_context.persist(persist_dir=knowledge_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define knowledge engine\n",
    "\n",
    "knowledge_engine = knowledge_index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a tool from the knowledge engine\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=knowledge_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"knowledge_database\",\n",
    "            description=(\n",
    "                \"Provides information about managing car dealership inventory.\"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 10:47:44,138 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 10:47:53,318 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Here is a list of parts that are marked as obsolete in the inventory:\n",
      "\n",
      "1. Part Number: 004-153, Description: bearing ntn 6203llu/2a 40x17x12, Quantity: 1, Price: $9.99\n",
      "2. Part Number: 01-0140, Description: valve stem seal, Quantity: 8, Price: $5.99\n",
      "3. Part Number: 01-04249, Description: shinko 804 big block 90/90-21, Quantity: 1, Price: $104.99\n",
      "4. Part Number: 010088hh, Description: ebc pad fa88hhferodo 310-, Quantity: 2, Price: $64.99\n",
      "5. Part Number: 010095hh, Description: ebc pad fa95hhferodo 310-, Quantity: 4, Price: $64.99\n",
      "6. ... (and more)\n",
      "\n",
      "There are a total of 9008 parts marked as obsolete in the inventory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 10:48:02,873 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 10:48:03,062 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 10:48:05,294 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 10:48:07,395 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: The obsolescence problem in this context is primarily caused by a surplus of obsolete parts in the automotive dealership's inventory. This surplus leads to wasted resources, financial losses, and difficulty in generating revenue. The obsolescence issue is often a result of inventory errors, low sales volume, and a high percentage of unsold special order parts, which can make up a significant portion of the obsolete parts in the department.\n"
     ]
    }
   ],
   "source": [
    "# define agent using both knowledge query engine tool and function tools\n",
    "# makes it so that the agent has access to both the knowledge database and the SQL data\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "agent = OpenAIAgent.from_tools(query_engine_tools + all_tools,\n",
    "                               llm=OpenAI(temperature=0.1, model=\"gpt-3.5-turbo-0125\"),\n",
    "                               verbose=False)\n",
    "\n",
    "while True:\n",
    "    text_input = input(\"User:\")\n",
    "    if text_input == \"exit\":\n",
    "        break\n",
    "    response = agent.chat(text_input)\n",
    "    print(f\"Agent: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 09:47:59,415 - WARNING - Ignoring wrong pointing object 22 0 (offset 0)\n",
      "2024-06-19 09:47:59,519 - WARNING - Ignoring wrong pointing object 20 0 (offset 0)\n",
      "2024-06-19 09:47:59,595 - WARNING - Ignoring wrong pointing object 8 0 (offset 0)\n",
      "2024-06-19 09:47:59,596 - WARNING - Ignoring wrong pointing object 10 0 (offset 0)\n",
      "2024-06-19 09:47:59,598 - WARNING - Ignoring wrong pointing object 13 0 (offset 0)\n",
      "2024-06-19 09:47:59,599 - WARNING - Ignoring wrong pointing object 16 0 (offset 0)\n",
      "2024-06-19 09:47:59,600 - WARNING - Ignoring wrong pointing object 18 0 (offset 0)\n",
      "2024-06-19 09:47:59,601 - WARNING - Ignoring wrong pointing object 21 0 (offset 0)\n",
      "2024-06-19 09:47:59,602 - WARNING - Ignoring wrong pointing object 29 0 (offset 0)\n",
      "2024-06-19 09:47:59,862 - WARNING - Ignoring wrong pointing object 6 0 (offset 0)\n",
      "2024-06-19 09:47:59,863 - WARNING - Ignoring wrong pointing object 8 0 (offset 0)\n",
      "2024-06-19 09:47:59,864 - WARNING - Ignoring wrong pointing object 10 0 (offset 0)\n",
      "2024-06-19 09:47:59,864 - WARNING - Ignoring wrong pointing object 23 0 (offset 0)\n",
      "2024-06-19 09:48:00,033 - WARNING - Ignoring wrong pointing object 9 0 (offset 0)\n",
      "2024-06-19 09:48:00,034 - WARNING - Ignoring wrong pointing object 11 0 (offset 0)\n",
      "2024-06-19 09:48:00,036 - WARNING - Ignoring wrong pointing object 17 0 (offset 0)\n",
      "2024-06-19 09:48:00,037 - WARNING - Ignoring wrong pointing object 21 0 (offset 0)\n",
      "2024-06-19 09:48:00,158 - WARNING - Ignoring wrong pointing object 17 0 (offset 0)\n",
      "2024-06-19 09:48:00,260 - WARNING - Ignoring wrong pointing object 6 0 (offset 0)\n",
      "2024-06-19 09:48:00,261 - WARNING - Ignoring wrong pointing object 8 0 (offset 0)\n",
      "2024-06-19 09:48:00,262 - WARNING - Ignoring wrong pointing object 10 0 (offset 0)\n",
      "2024-06-19 09:48:00,263 - WARNING - Ignoring wrong pointing object 12 0 (offset 0)\n",
      "2024-06-19 09:48:00,263 - WARNING - Ignoring wrong pointing object 15 0 (offset 0)\n",
      "2024-06-19 09:48:00,264 - WARNING - Ignoring wrong pointing object 21 0 (offset 0)\n",
      "2024-06-19 09:48:00,690 - WARNING - Ignoring wrong pointing object 8 0 (offset 0)\n",
      "2024-06-19 09:48:00,692 - WARNING - Ignoring wrong pointing object 23 0 (offset 0)\n",
      "2024-06-19 09:48:03,913 - WARNING - Ignoring wrong pointing object 19 0 (offset 0)\n",
      "2024-06-19 09:48:03,967 - WARNING - Ignoring wrong pointing object 7 0 (offset 0)\n",
      "2024-06-19 09:48:03,967 - WARNING - Ignoring wrong pointing object 14 0 (offset 0)\n",
      "2024-06-19 09:48:03,967 - WARNING - Ignoring wrong pointing object 16 0 (offset 0)\n",
      "2024-06-19 09:48:03,967 - WARNING - Ignoring wrong pointing object 30 0 (offset 0)\n",
      "2024-06-19 09:48:03,977 - WARNING - Ignoring wrong pointing object 32 0 (offset 0)\n",
      "2024-06-19 09:48:03,978 - WARNING - Ignoring wrong pointing object 34 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load file C:\\Users\\vivia\\co-pilot-v1\\data\\knowledge_database\\~$le_inventory_trend.docx with error: File is not a zip file. Skipping...\n",
      "Failed to load file C:\\Users\\vivia\\co-pilot-v1\\data\\knowledge_database\\~$le_stock.docx with error: File is not a zip file. Skipping...\n"
     ]
    }
   ],
   "source": [
    "# CODE FOR HYBRID QUERY ENGINE THAT WASN'T WORKING\n",
    "# file reader for knowledge database\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents_dir = r\"C:\\Users\\vivia\\co-pilot-v1\\data\\knowledge_database\"\n",
    "reader = SimpleDirectoryReader(input_dir=documents_dir)\n",
    "knowledge_documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data from function tools\n",
    "tools_analysis_results = compile_analysis_results()\n",
    "\n",
    "# Converting dataframes into csvs\n",
    "for key in tools_analysis_results.keys():\n",
    "    base_path = r\"C:\\Users\\vivia\\co-pilot-v1\\Notebooks\\tools_data\\\\\"\n",
    "    file_name = key + \".csv\"\n",
    "    tools_analysis_results[key].to_csv(base_path + file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in csvs into documents\n",
    "tools_documents_dir = r\"C:\\Users\\vivia\\co-pilot-v1\\Notebooks\\tools_data\"\n",
    "reader = SimpleDirectoryReader(input_dir=tools_documents_dir)\n",
    "tools_documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump data into json document\n",
    "# json.dump(tools_analysis_results, out_file, indent=6)\n",
    "\n",
    "# out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json reader for tools data\n",
    "# from llama_index.readers.json import JSONReader\n",
    "# json_file = r\"C:\\Users\\vivia\\co-pilot-v1\\Notebooks\\index\\tools_data.json\"\n",
    "# reader = JSONReader()\n",
    "# keyword_documents = reader.load_data(input_file=json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "# from llama_index.core.node_parser import JSONNodeParser\n",
    "\n",
    "# parser = JSONNodeParser()\n",
    "knowledge_nodes = Settings.node_parser.get_nodes_from_documents(knowledge_documents)\n",
    "tools_nodes = Settings.node_parser.get_nodes_from_documents(tools_documents)\n",
    "# json_nodes = parser.get_nodes_from_documents(json_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_storage_context = StorageContext.from_defaults()\n",
    "vector_storage_context.docstore.add_documents(knowledge_nodes)\n",
    "\n",
    "keyword_storage_context = StorageContext.from_defaults()\n",
    "keyword_storage_context.docstore.add_documents(tools_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 09:56:28,715 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 09:56:30,032 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 09:56:32,444 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 09:56:33,751 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 09:56:35,667 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 09:56:36,865 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 09:56:38,465 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 09:56:40,552 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 09:56:40,967 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.EMBEDDING ->  1.383237 seconds\n",
      "    |_CBEventType.EMBEDDING ->  1.35824 seconds\n",
      "    |_CBEventType.EMBEDDING ->  2.356682 seconds\n",
      "    |_CBEventType.EMBEDDING ->  2.1506 seconds\n",
      "    |_CBEventType.EMBEDDING ->  1.095168 seconds\n",
      "    |_CBEventType.EMBEDDING ->  1.173531 seconds\n",
      "    |_CBEventType.EMBEDDING ->  2.530428 seconds\n",
      "    |_CBEventType.EMBEDDING ->  1.184701 seconds\n",
      "    |_CBEventType.EMBEDDING ->  0.181473 seconds\n",
      "**********\n",
      "**********\n",
      "Trace: index_construction\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleKeywordTableIndex, VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(tools_nodes, storage_context=keyword_storage_context)\n",
    "keyword_index = SimpleKeywordTableIndex(tools_nodes, storage_context=keyword_storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.core.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    "    KeywordTableSimpleRetriever,\n",
    ")\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        keyword_retriever: KeywordTableSimpleRetriever,\n",
    "        mode: str = \"AND\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._keyword_retriever = keyword_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(keyword_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# define custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=2)\n",
    "keyword_retriever = KeywordTableSimpleRetriever(index=keyword_index)\n",
    "custom_retriever = CustomRetriever(vector_retriever, keyword_retriever)\n",
    "\n",
    "# define response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "custom_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# vector query engine\n",
    "vector_query_engine = RetrieverQueryEngine(\n",
    "    retriever=vector_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "# keyword query engine\n",
    "keyword_query_engine = RetrieverQueryEngine(\n",
    "    retriever=keyword_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 09:57:01,516 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-06-19 09:57:01,602 - INFO - > Starting query: What are some parts from the roi csv\n",
      "2024-06-19 09:57:01,602 - INFO - query keywords: ['roi', 'parts', 'csv']\n",
      "2024-06-19 09:57:01,602 - INFO - > Extracted keywords: ['parts']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  0.284426 seconds\n",
      "**********\n",
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "response = custom_query_engine.query(\"What are some parts from the roi csv\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PartsWise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
