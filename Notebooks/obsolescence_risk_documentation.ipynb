{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Obsolescence Risk Machine Learning Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load and Prepare the Dataset__\n",
    "The dataset is loaded from a Feather file. Obsolete items are excluded based on months_no_sale, and a new binary target variable obsolescence_dummy is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(\"/Users/skylerwilson/Desktop/PartsWise/Data/Processed/parts_data.feather\")\n",
    "\n",
    "# Exclude obsolete items and assign obsolescence_dummy\n",
    "df = df[df['months_no_sale'] < 12].copy()\n",
    "df['obsolescence_dummy'] = np.where(df['months_no_sale'] >= 6, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ Segment data using a 6 month binary classifier because 6 months would represent the 50% mark for obsolescence risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define Features and Target Variable:__ The features and target variable are defined, excluding columns irrelevant to the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['part_number', 'description', 'supplier_name', \n",
    "            'sales_to_stock_ratio', 'reorder_point', 'sales_last_jan', 'sales_last_feb',\n",
    "            'sales_last_mar', 'sales_last_apr', 'sales_last_may', 'sales_last_jun',\n",
    "            'sales_last_jul', 'sales_last_aug', 'sales_last_sep', 'sales_last_oct',\n",
    "            'sales_last_nov', 'sales_last_dec', 'sales_jan', 'sales_feb',\n",
    "            'sales_mar', 'sales_apr', 'sales_may', 'sales_jun', 'sales_jul',\n",
    "            'sales_aug', 'sales_sep', 'sales_oct', 'sales_nov', 'sales_dec',\n",
    "            'sales_revenue', 'cogs', 'cost_per_unit', 'rolling_12_month_sales',\n",
    "            'price', 'safety_stock', 'months_no_sale']\n",
    "\n",
    "X = df.drop(columns=drop_cols + ['obsolescence_dummy'])\n",
    "y = df['obsolescence_dummy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ Define the target variable and remove any columns that have high multicolinierity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Split the Data:__ The dataset is split into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ ensure no data leakage and that the model works well on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Preprocessing:__ Numerical features are scaled and transformed. SMOTE is applied to handle class imbalance. SMOTE artifically created more instances of the 0 class because there was a large imbalance toward the 1 class due to a large amount of obsolete parts in inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('power_trans', PowerTransformer(method='yeo-johnson'))]),\n",
    "     numerical_features)])\n",
    "\n",
    "# Apply preprocessing to training data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Apply SMOTE after preprocessing\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_rs, y_train_rs = smote.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "X_val_transformed = preprocessor.transform(X_val)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ preprocess the data to a format thats better for classification ML models\n",
    "\n",
    "-  __Robust scalar:__ removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n",
    "-  __column transformer:__ ensures all numerical features are of type int or float\n",
    "-  __Yeo-johnson Transformation:__ inflates low variance data and deflates high variance data to create a more uniform dataset \n",
    "- __SMOTE:__ an oversampling technique that generates synthetic samples from the minority class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hyperparameter Tuning:__ Hyperparameters for the RandomForestClassifier are defined and optimized using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'n_estimators': np.arange(200, 501, 100),\n",
    "    'max_depth': list(np.arange(2, 10, 1)),  # Reduced upper limit\n",
    "    'min_samples_split': np.arange(5, 25, 5),  # Increased the lower limit\n",
    "    'min_samples_leaf': np.arange(5, 25, 5),  # Increased the lower limit\n",
    "    'max_features': ['sqrt', 'log2'],  # Added a fixed fraction\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=1000,  # You can adjust the number of iterations\n",
    "    cv=5,  # Cross-validation setting\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_rs, y_train_rs)\n",
    "\n",
    "# Best model from grid search\n",
    "best_model = random_search.best_estimator_\n",
    "print(best_model)\n",
    "best_params = random_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hyperparameters:__\n",
    "-  __n_estimators:__ This represents the number of trees in the random forest. A higher number of trees generally improves the performance but also increases the computational cost. Here, it's set to a range from 200 to 500, with steps of 100.\n",
    "-  __max_depth:__ This parameter determines the maximum depth of each tree in the forest. A deeper tree can model more complex patterns but might also lead to overfitting. The range here is set from 2 to 10.\n",
    "-  __min_samples_split:__ This parameter specifies the minimum number of samples required to split an internal node. Setting a higher value can prevent the model from learning overly specific patterns (overfitting). The range here is from 5 to 25, with steps of 5.\n",
    "-  __min_samples_leaf:__ This represents the minimum number of samples required to be at a leaf node. This helps control overfitting by ensuring that leaf nodes have a minimum number of observations. The range is from 5 to 25, with steps of 5.\n",
    "-  __max_features:__ This parameter specifies the number of features to consider when looking for the best split. Common values are 'sqrt' (square root of the number of features) and 'log2' (log base 2 of the number of features). These values help reduce the correlation between trees in the forest.\n",
    "-  __bootstrap:__ This boolean parameter determines whether bootstrap samples are used when building trees. If set to True, each tree is built from a random sample with replacement from the training set.\n",
    "\n",
    "__Randomized Search__: performs hyperparameter optimization by randomly sampling from the defined parameter distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature Selection:__ Recursive feature elimination with cross-validation (RFECV) is used to select the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = RFECV(estimator=best_model, step=1, cv=StratifiedKFold(10), scoring='f1')\n",
    "selector = selector.fit(X_train_rs, y_train_rs)\n",
    "\n",
    "# Get the selected features\n",
    "feature_names = X_train.columns.tolist()  # Get the feature names from your DataFrame\n",
    "selected_features = [feature_names[i] for i in range(len(feature_names)) if selector.support_[i]]\n",
    "\n",
    "# Transform training and test data to keep only selected features\n",
    "X_train_selected = selector.transform(X_train_rs)\n",
    "X_val_selected = selector.transform(X_val_transformed)\n",
    "X_test_selected = selector.transform(X_test_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ Perform feature selection to remove insignificant features and help prevent overfitting\n",
    "\n",
    "__RECV:__ Recursive feature elimination with cross-validation to select features based on the average score change in the scorer after cross valiadation\n",
    "\n",
    "__Stratified K Fold:__ Cross validation technique where folds are made by preserving the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Training and Calibration:__ The best model is trained on the selected features and calibrated using validation data. Calibration improves performance of the predicted probabilities generated by the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train your classifier with the selected features\n",
    "final_model = best_model.fit(X_train_selected, y_train_rs)\n",
    "\n",
    "# Calibrate the model on the validation set using the selected features\n",
    "calibrator = CalibratedClassifierCV(final_model, cv='prefit', method='sigmoid')\n",
    "calibrated_model = calibrator.fit(X_val_selected.astype(np.float64), y_val.astype(np.float64))\n",
    "\n",
    "# Predict probabilities on the test set using the selected features\n",
    "y_test_proba = calibrated_model.predict_proba(X_test_selected)[:, 1].astype(np.float64)\n",
    "\n",
    "calibrator_ = CalibratedClassifierCV(final_model, cv='prefit', method='isotonic')\n",
    "calibrated_model_ = calibrator_.fit(X_val_selected.astype(np.float64), y_val.astype(np.float64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ predicted probabilities are adjusted to be more representative of the true likelihood of an event. \n",
    "\n",
    "-  __Sigmoid:__ fits a logistic regression model to the scores output by the classifier. This method assumes that the relationship between the predicted probabilities and the true likelihoods follows a sigmoid curve.\n",
    "-  __Isotonic:__ non-parametric calibration method that fits a piecewise constant non-decreasing function to the predicted probabilities."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
