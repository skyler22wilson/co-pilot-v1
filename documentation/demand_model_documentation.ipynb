{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Demand Prediction Model Documentation__\n",
    "This documentation provides a detailed overview of the data preprocessing, feature selection, model training, and evaluation processes used to build a demand prediction model using XGBoost. Each step is meticulously designed to handle the complexities of the data and optimize the model's predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data loading:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_feather(\"/Users/skylerwilson/Desktop/PartsWise/Data/Processed/parts_data.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose__: Load the dataset from a Feather file for efficient data reading and processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature and Target Selection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features (X) and target (y) for modeling\n",
    "feature_cols = [col for col in df.columns if col not in {'part_number', 'description', 'supplier_name',\n",
    "                                                         'sales_last_jan','sales_last_feb', 'sales_last_mar', 'sales_last_apr', 'sales_last_may',\n",
    "                                                         'sales_last_jun', 'sales_last_jul', 'sales_last_aug', 'sales_last_sep',\n",
    "                                                         'sales_last_oct', 'sales_last_nov', 'sales_last_dec', 'sales_jan',\n",
    "                                                         'sales_feb', 'sales_mar', 'sales_apr', 'sales_may', 'sales_jun', \n",
    "                                                         'sales_jul', 'sales_aug', 'sales_sep', 'sales_oct', 'sales_nov', \n",
    "                                                         'sales_dec', 'sales_this_year', 'sales_last_year', 'sales_revenue',\n",
    "                                                         'price', 'sales_to_stock_ratio', 'rolling_12_month_sales', 'cogs',\n",
    "                                                         'margin', 'quantity', 'demand'}]\n",
    "X = df[feature_cols]\n",
    "y = df['rolling_12_month_sales']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ Select features (X) excluding non-relevant columns and set the target variable (y) as rolling_12_month_sales. Rolling 12 month sales is used to guage demand for each part based on the number of sales on a rolling basis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train-Test Split:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ Split the dataset into training (70%) and testing (30%) sets to evaluate model performance on unseen data.\n",
    "__Hyperparameters:__\n",
    "test_size=0.3: 30% of the data is used for testing.\n",
    "random_state=42: Ensures reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocessing Pipeline:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', RobustScaler()),\n",
    "            ('power_trans', PowerTransformer(method='yeo-johnson'))]),\n",
    "        numerical_features)\n",
    "    ])\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ Apply robust scaling and power transformation to numerical features.\n",
    "-  __Robust scalar:__ removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n",
    "-  __Yeo-johnson Transformation:__ inflates low variance data and deflates high variance data to create a more uniform dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hyperparameter Space for Hyperopt__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "space = {\n",
    "    'objective': 'reg:pseudohubererror',\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0.25, 1.0),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.05)),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 15, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 3, 15, 1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 350, 750, 10),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.0001), np.log(1)),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', np.log(1), np.log(3)),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'max_delta_step': hp.quniform('max_delta_step', 5, 10, 1),\n",
    "    'huber_slope': hp.uniform('huber_slope', 0.2, 0.3),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ Define the search space for hyperparameter optimization using Hyperopt.\n",
    "__Hyperparameters:__\n",
    "-  objective: 'reg:pseudohubererror'\n",
    "-  colsample_bytree: Fraction of features to consider for each tree.\n",
    "-  gamma: Minimum loss reduction required to make a further partition.\n",
    "-  learning_rate: Step size shrinkage used to prevent overfitting.\n",
    "-  max_depth: Maximum depth of a tree.\n",
    "-  min_child_weight: Minimum sum of instance weight needed in a child.\n",
    "-  n_estimators: Number of boosting rounds.\n",
    "-  reg_alpha: L1 regularization term on weights.\n",
    "-  reg_lambda: L2 regularization term on weights.\n",
    "-  subsample: Fraction of samples to be used for each tree.\n",
    "-  max_delta_step: Maximum delta step we allow each tree's weight estimate to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__KFold Cross-Validation:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ Use KFold cross-validation to evaluate the model's performance.\n",
    "\n",
    "__Hyperparameters:__\n",
    "-  n_splits=5: Number of folds.\n",
    "-  shuffle=True: Shuffle the data before splitting into folds.\n",
    "-  random_state=42: Ensures reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Objective Function for Hyperopt:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "def objective(params):\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    params['max_delta_step'] = int(params['max_delta_step']) \n",
    "    \n",
    "    model = XGBRegressor(**params)\n",
    "    scores = cross_val_score(model, X_train_transformed, y_train, scoring='neg_mean_absolute_error', cv=5)\n",
    "    return {'loss': -scores.mean(), 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ Define the objective function for Hyperopt to minimize the negative mean absolute error (MAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hyperparameter Optimization with Hyperopt__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "\n",
    "trials = Trials()\n",
    "best_hyperparams = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=500, trials=trials)\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ Perform hyperparameter optimization using the Tree-structured Parzen Estimator (TPE) algorithm.\n",
    "__Hyperparameters:__\n",
    "__max_evals=500:__ Maximum number of evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Recursive Feature Elimination with Cross-Validation (RFECV):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rfecv = RFECV(estimator=model, step=1, cv=KFold(10), scoring='neg_mean_absolute_error')\n",
    "rfecv.fit(X_train_transformed, y_train)\n",
    "selected_features_mask = rfecv.support_\n",
    "feature_ranking = rfecv.ranking_\n",
    "selected_features = [feature for feature, selected in zip(numerical_features, selected_features_mask) if selected]\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ Perform feature selection using RFECV to select the best subset of features.\n",
    "\n",
    "__Hyperparameters:__\n",
    "\n",
    "__step=1:__ Number of features to remove at each iteration.\n",
    "\n",
    "__cv=KFold(10):__ 10-fold cross-validation.\n",
    "\n",
    "__scoring='neg_mean_absolute_error':__ Scoring metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Training with Best Hyperparameters:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_transformed_rfe, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ Train the XGBoost model using the best hyperparameters and selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_pred = model.predict(X_test_transformed_rfe)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'\\nModel Performance')\n",
    "print(f\"Test MSE: {mse}\")\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "print(f\"Test MAE: {mae}\")\n",
    "print(f\"Test R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ Evaluate the model's performance using various metrics.\n",
    "__Metrics:__\n",
    "\n",
    "__Mean Squared Error (MSE):__ MSE measures the average of the squares of the errors, which is the difference between the actual and predicted values.\n",
    "Purpose: It provides an idea of how close the predicted values are to the actual values. Lower MSE indicates better model performance.\n",
    "\n",
    "__Root Mean Squared Error (RMSE):__ square root of MSE and provides an error metric in the same units as the target variable.\n",
    "\n",
    "__Mean Absolute Error (MAE):__ MAE measures the average magnitude of the errors in a set of predictions, without considering their direction.\n",
    "\n",
    "__R² Score:__ measures how well the regression predictions approximate the real data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
